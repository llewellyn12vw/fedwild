{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from wildlife_datasets.datasets import LeopardID2022\n",
    "\n",
    "def analyze_and_distribute_rarity(source_df, num_clients=7, richness_sections=5, rare_threshold_percentile=20, \n",
    "                                  test_set_percentage=0.3, output_folder='client_rarity_data', min_samples_threshold=5):\n",
    "    \"\"\"\n",
    "    Analyze identity counts and distribute them based on rarity and richness levels\n",
    "    \n",
    "    Args:\n",
    "        source_df: Source DataFrame with columns like ['image_id', 'identity', 'path', ...]\n",
    "        num_clients: Number of clients to distribute to\n",
    "        richness_sections: Number of richness sections to divide data into\n",
    "        rare_threshold_percentile: Percentile threshold for rare cases (lower = more rare)\n",
    "        test_set_percentage: Percentage of samples for test set (query + gallery)\n",
    "        output_folder: Folder name to save client data\n",
    "        min_samples_threshold: Minimum samples per identity (filter out below this)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with client assignments and rarity classifications\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create identity counts from source DataFrame\n",
    "    identity_counts = source_df['identity'].value_counts()\n",
    "    \n",
    "    # Remove 'unknown' entries if present\n",
    "    identity_counts = identity_counts[identity_counts.index != 'unknown']\n",
    "    \n",
    "    # Filter out identities below minimum threshold\n",
    "    print(f\"Before filtering: {len(identity_counts)} identities\")\n",
    "    identity_counts = identity_counts[identity_counts >= min_samples_threshold]\n",
    "    print(f\"After filtering (>= {min_samples_threshold} samples): {len(identity_counts)} identities\")\n",
    "    \n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame({'identity_id': identity_counts.index, 'sample_count': identity_counts.values})\n",
    "    \n",
    "    print(f\"Total samples: {df['sample_count'].sum()}\")\n",
    "    print(f\"Sample count statistics:\")\n",
    "    print(df['sample_count'].describe())\n",
    "    \n",
    "    # Determine rarity threshold\n",
    "    rare_threshold = np.percentile(df['sample_count'], rare_threshold_percentile)\n",
    "    print(f\"\\nRare threshold (bottom {rare_threshold_percentile}%): <= {rare_threshold} samples\")\n",
    "    \n",
    "    # Classify rarity\n",
    "    def classify_rarity(count):\n",
    "        if count <= rare_threshold:\n",
    "            return 'rare'\n",
    "        elif count <= np.percentile(df['sample_count'], 50):\n",
    "            return 'common'\n",
    "        else:\n",
    "            return 'abundant'\n",
    "    \n",
    "    df['rarity_class'] = df['sample_count'].apply(classify_rarity)\n",
    "    \n",
    "    # Create richness sections based on sample count quantiles\n",
    "    richness_boundaries = np.percentile(df['sample_count'], \n",
    "                                      np.linspace(0, 100, richness_sections + 1))\n",
    "    \n",
    "    def classify_richness(count):\n",
    "        for i in range(len(richness_boundaries) - 1):\n",
    "            if richness_boundaries[i] <= count <= richness_boundaries[i + 1]:\n",
    "                return f'section_{i + 1}'  # section_1 = poorest, section_n = richest\n",
    "        return f'section_{richness_sections}'\n",
    "    \n",
    "    df['richness_section'] = df['sample_count'].apply(classify_richness)\n",
    "    \n",
    "    # Display distribution\n",
    "    print(f\"\\nRarity distribution:\")\n",
    "    print(df['rarity_class'].value_counts())\n",
    "    print(f\"\\nRichness section distribution:\")\n",
    "    print(df['richness_section'].value_counts())\n",
    "    \n",
    "    # Create client richness levels (some poor, some medium, some rich)\n",
    "    client_richness = {}\n",
    "    clients_per_section = max(1, num_clients // richness_sections)\n",
    "    \n",
    "    for i in range(richness_sections):\n",
    "        section_name = f'section_{i + 1}'\n",
    "        start_client = i * clients_per_section\n",
    "        end_client = min((i + 1) * clients_per_section, num_clients)\n",
    "        \n",
    "        for client_id in range(start_client, end_client):\n",
    "            client_richness[client_id] = section_name\n",
    "    \n",
    "    # Handle remaining clients (assign to richest section)\n",
    "    for client_id in range(len(client_richness), num_clients):\n",
    "        client_richness[client_id] = f'section_{richness_sections}'\n",
    "    \n",
    "    print(f\"\\nClient richness assignments:\")\n",
    "    for client_id, richness in client_richness.items():\n",
    "        print(f\"Client {client_id}: {richness}\")\n",
    "    \n",
    "    # Initialize client assignments\n",
    "    client_assignments = defaultdict(list)\n",
    "    \n",
    "    # Separate identities by rarity\n",
    "    rare_identities = df[df['rarity_class'] == 'rare'].copy()\n",
    "    non_rare_identities = df[df['rarity_class'] != 'rare'].copy()\n",
    "    \n",
    "    print(f\"\\nDistributing {len(rare_identities)} rare identities...\")\n",
    "    \n",
    "    # Distribute rare identities: 70% to poor clients, 30% to rich clients\n",
    "    np.random.seed(42)\n",
    "    rare_identities = rare_identities.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    poor_clients = [cid for cid, richness in client_richness.items() if richness == 'section_1']\n",
    "    rich_clients = [cid for cid, richness in client_richness.items() if richness == f'section_{richness_sections}']\n",
    "    \n",
    "    # Distribute rare cases\n",
    "    for i, (_, identity_row) in enumerate(rare_identities.iterrows()):\n",
    "        if i < len(rare_identities) * 0.7:  # 70% to poor clients\n",
    "            if poor_clients:\n",
    "                client_id = poor_clients[i % len(poor_clients)]\n",
    "            else:\n",
    "                client_id = i % num_clients\n",
    "        else:  # 30% to rich clients\n",
    "            if rich_clients:\n",
    "                client_id = rich_clients[i % len(rich_clients)]\n",
    "            else:\n",
    "                client_id = i % num_clients\n",
    "        \n",
    "        client_assignments[client_id].append(identity_row)\n",
    "    \n",
    "    print(f\"Distributing {len(non_rare_identities)} non-rare identities...\")\n",
    "    \n",
    "    # Distribute non-rare identities using weighted distribution\n",
    "    # Rich clients get more identities, poor clients get fewer\n",
    "    client_weights = []\n",
    "    for client_id in range(num_clients):\n",
    "        richness = client_richness[client_id]\n",
    "        section_num = int(richness.split('_')[1])\n",
    "        # Higher section number = richer = higher weight\n",
    "        weight = section_num ** 2  # Exponential weighting\n",
    "        client_weights.append(weight)\n",
    "    \n",
    "    # Normalize weights\n",
    "    client_weights = np.array(client_weights)\n",
    "    client_weights = client_weights / client_weights.sum()\n",
    "    \n",
    "    # Distribute non-rare identities\n",
    "    non_rare_identities = non_rare_identities.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    for _, identity_row in non_rare_identities.iterrows():\n",
    "        client_id = np.random.choice(num_clients, p=client_weights)\n",
    "        client_assignments[client_id].append(identity_row)\n",
    "    \n",
    "    # Create final assignment DataFrame\n",
    "    assignment_records = []\n",
    "    for client_id, identities in client_assignments.items():\n",
    "        for identity_row in identities:\n",
    "            assignment_records.append({\n",
    "                'client_id': client_id,\n",
    "                'identity_id': identity_row['identity_id'],\n",
    "                'sample_count': identity_row['sample_count'],\n",
    "                'rarity_class': identity_row['rarity_class'],\n",
    "                'richness_section': identity_row['richness_section'],\n",
    "                'client_richness': client_richness[client_id]\n",
    "            })\n",
    "    \n",
    "    assignment_df = pd.DataFrame(assignment_records)\n",
    "    \n",
    "    # Display final distribution\n",
    "    print(f\"\\nFinal client distribution:\")\n",
    "    client_stats = assignment_df.groupby('client_id').agg({\n",
    "        'identity_id': 'count',\n",
    "        'sample_count': 'sum',\n",
    "        'rarity_class': lambda x: (x == 'rare').sum()\n",
    "    }).rename(columns={\n",
    "        'identity_id': 'num_identities',\n",
    "        'sample_count': 'total_samples',\n",
    "        'rarity_class': 'rare_count'\n",
    "    })\n",
    "    \n",
    "    client_stats['client_richness'] = [client_richness[cid] for cid in client_stats.index]\n",
    "    client_stats['rare_percentage'] = (client_stats['rare_count'] / client_stats['num_identities'] * 100).round(2)\n",
    "    \n",
    "    print(client_stats)\n",
    "    \n",
    "    # Create client folders and CSV files\n",
    "    create_client_folders(assignment_df, source_df, test_set_percentage, output_folder)\n",
    "    \n",
    "    return assignment_df, client_stats\n",
    "\n",
    "def create_client_folders(assignment_df, source_df, test_set_percentage, output_folder):\n",
    "    \"\"\"\n",
    "    Create client folders with train, query, and gallery CSV files matching source structure\n",
    "    \"\"\"\n",
    "    # Create main output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nCreating client folders in: {output_folder}\")\n",
    "    \n",
    "    for client_id in assignment_df['client_id'].unique():\n",
    "        client_data = assignment_df[assignment_df['client_id'] == client_id].copy()\n",
    "        \n",
    "        # Create client folder\n",
    "        client_folder = os.path.join(output_folder, str(client_id))\n",
    "        os.makedirs(client_folder, exist_ok=True)\n",
    "        \n",
    "        train_records = []\n",
    "        query_records = []\n",
    "        gallery_records = []\n",
    "        \n",
    "        for _, identity_row in client_data.iterrows():\n",
    "            identity_id = identity_row['identity_id']\n",
    "            total_samples = identity_row['sample_count']\n",
    "            rarity_class = identity_row['rarity_class']\n",
    "            \n",
    "            # Get all samples for this identity from source data\n",
    "            identity_samples = source_df[source_df['identity'] == identity_id].copy()\n",
    "            identity_samples = identity_samples.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            # Calculate splits ensuring minimum requirements\n",
    "            test_set_size = max(3, int(total_samples * test_set_percentage))  # Minimum 3 for test\n",
    "            train_size = total_samples - test_set_size\n",
    "            \n",
    "            # Ensure minimum 2 training samples\n",
    "            if train_size < 2:\n",
    "                train_size = 2\n",
    "                test_set_size = total_samples - train_size\n",
    "            \n",
    "            # For test set: minimum 1 query, 2 gallery\n",
    "            if test_set_size >= 3:\n",
    "                query_size = 1\n",
    "                gallery_size = max(2, test_set_size - query_size)\n",
    "            else:\n",
    "                # If not enough for minimum, skip this identity\n",
    "                print(f\"Warning: Identity {identity_id} has only {total_samples} samples, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Split the actual samples\n",
    "            train_samples = identity_samples[:train_size].copy()\n",
    "            query_samples = identity_samples[train_size:train_size + query_size].copy()\n",
    "            gallery_samples = identity_samples[train_size + query_size:train_size + query_size + gallery_size].copy()\n",
    "            \n",
    "            # Add rarity class to all samples\n",
    "            train_samples['rarity_class'] = rarity_class\n",
    "            query_samples['rarity_class'] = rarity_class\n",
    "            gallery_samples['rarity_class'] = rarity_class\n",
    "            \n",
    "            train_records.append(train_samples)\n",
    "            query_records.append(query_samples)\n",
    "            gallery_records.append(gallery_samples)\n",
    "        \n",
    "        # Create DataFrames and save CSV files\n",
    "        if train_records:\n",
    "            train_df = pd.concat(train_records, ignore_index=True)\n",
    "            query_df = pd.concat(query_records, ignore_index=True)\n",
    "            gallery_df = pd.concat(gallery_records, ignore_index=True)\n",
    "            \n",
    "            # Save CSV files\n",
    "            train_df.to_csv(os.path.join(client_folder, 'train.csv'), index=False)\n",
    "            query_df.to_csv(os.path.join(client_folder, 'query.csv'), index=False)\n",
    "            gallery_df.to_csv(os.path.join(client_folder, 'gallery.csv'), index=False)\n",
    "            \n",
    "            print(f\"Client {client_id}:\")\n",
    "            print(f\"  Train: {len(train_df)} samples\")\n",
    "            print(f\"  Query: {len(query_df)} samples\") \n",
    "            print(f\"  Gallery: {len(gallery_df)} samples\")\n",
    "            print(f\"  Test percentage: {((len(query_df) + len(gallery_df)) / (len(train_df) + len(query_df) + len(gallery_df)) * 100):.1f}%\")\n",
    "            \n",
    "            # Verify rarity distribution\n",
    "            rare_train = (train_df['rarity_class'] == 'rare').sum()\n",
    "            rare_total = (train_df['rarity_class'] == 'rare').sum() + (query_df['rarity_class'] == 'rare').sum() + (gallery_df['rarity_class'] == 'rare').sum()\n",
    "            print(f\"  Rare cases: {rare_total} total, {rare_train} in training\")\n",
    "        else:\n",
    "            print(f\"Warning: Client {client_id} has no valid data after filtering\")\n",
    "    \n",
    "    print(f\"\\nClient folders created successfully in: {output_folder}\")\n",
    "\n",
    "\n",
    "# Example usage with the provided data\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset = LeopardID2022('/home/wellvw12/leopard')\n",
    "    source_df = dataset.df\n",
    "    \n",
    "    print(f\"Loaded dataset with {len(source_df)} samples\")\n",
    "    print(f\"Dataset columns: {source_df.columns.tolist()}\")\n",
    "    \n",
    "    # Run the analysis\n",
    "    assignment_df, client_stats = analyze_and_distribute_rarity(\n",
    "        source_df, \n",
    "        num_clients=7, \n",
    "        richness_sections=4, \n",
    "        rare_threshold_percentile=10,\n",
    "        test_set_percentage=0.1,\n",
    "        output_folder='client_rarity_data9ß',\n",
    "        min_samples_threshold=5\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    assignment_df.to_csv('/home/wellvw12/fedReID/client_assignments_with_rarity.csv', index=False)\n",
    "    client_stats.to_csv('/home/wellvw12/fedReID/client_statistics.csv')\n",
    "    \n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"- Client assignments: /home/wellvw12/fedReID/client_assignments_with_rarity.csv\")\n",
    "    print(f\"- Client statistics: /home/wellvw12/fedReID/client_statistics.csv\")\n",
    "    print(f\"- Client folders: /home/wellvw12/fedReID/client_rarity_data_9/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
