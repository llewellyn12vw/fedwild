{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1eb0f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from wildlife_tools.tools import realize\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def set_random_states(states):\n",
    "    if 'os_rng_state' in states and states[\"os_rng_state\"]:\n",
    "        os.environ[\"PYTHONHASHSEED\"] = states[\"os_rng_state\"]\n",
    "    if 'random_rng_state' in states:\n",
    "        random.setstate(states[\"random_rng_state\"])\n",
    "    if 'numpy_rng_state' in states:\n",
    "        np.random.set_state(states[\"numpy_rng_state\"])\n",
    "    if 'torch_rng_state' in states:\n",
    "        torch.set_rng_state(states[\"torch_rng_state\"])\n",
    "    if 'torch_cuda_rng_state' in states:\n",
    "        torch.cuda.set_rng_state(states[\"torch_cuda_rng_state\"])\n",
    "    if 'torch_cuda_rng_state_all' in states:\n",
    "        torch.cuda.set_rng_state_all(states[\"torch_cuda_rng_state_all\"])\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_random_states():\n",
    "    states = {}\n",
    "    states[\"os_rng_state\"] = os.environ.get(\"PYTHONHASHSEED\")\n",
    "    states[\"random_rng_state\"] = random.getstate()\n",
    "    states[\"numpy_rng_state\"] = np.random.get_state()\n",
    "    states[\"torch_rng_state\"] = torch.get_rng_state()\n",
    "    if torch.cuda.is_available():\n",
    "        states[\"torch_cuda_rng_state\"] = torch.cuda.get_rng_state()\n",
    "        states[\"torch_cuda_rng_state_all\"] = torch.cuda.get_rng_state_all()\n",
    "    return states\n",
    "\n",
    "\n",
    "class BasicTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        model,\n",
    "        objective,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        scheduler=None,\n",
    "        device=\"cuda\",\n",
    "        batch_size=128,\n",
    "        num_workers=1,\n",
    "        accumulation_steps=1,\n",
    "        epoch_callback=None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.model = model.to(device)\n",
    "        self.objective = objective.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.epochs = epochs\n",
    "        self.epoch = 0\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.epoch_callback = epoch_callback\n",
    "\n",
    "    def train(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            epoch_data = self.train_epoch(loader)\n",
    "            self.epoch += 1\n",
    "\n",
    "            if self.epoch_callback:\n",
    "                self.epoch_callback(trainer=self, epoch_data=epoch_data)\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        model = self.model.train()\n",
    "        losses = []\n",
    "        for i, batch in enumerate(\n",
    "            tqdm(loader, desc=f\"Epoch {self.epoch}: \", mininterval=1, ncols=100)\n",
    "        ):\n",
    "            x, y = batch\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "\n",
    "            out = model(x)\n",
    "            loss = self.objective(out, y)\n",
    "            loss.backward()\n",
    "            if (i - 1) % self.accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            losses.append(loss.detach().cpu())\n",
    "\n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "        print('loss',loss)\n",
    "        return {\"train_loss_epoch_avg\": np.mean(losses)}\n",
    "\n",
    "    def save(self, folder, file_name=\"checkpoint.pth\", save_rng=True, **kwargs):\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        checkpoint = {}\n",
    "        checkpoint[\"model\"] = self.model.state_dict()\n",
    "        checkpoint[\"objective\"] = self.objective.state_dict()\n",
    "        checkpoint[\"optimizer\"] = self.optimizer.state_dict()\n",
    "        checkpoint[\"epoch\"] = self.epoch\n",
    "        if save_rng:\n",
    "            checkpoint[\"rng_states\"] = get_random_states()\n",
    "        if self.scheduler:\n",
    "            checkpoint[\"scheduler\"] = self.scheduler.state_dict()\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(folder, file_name))\n",
    "\n",
    "    def load(self, path, load_rng=True):\n",
    "        checkpoint = torch.load(path, map_location=torch.device(self.device))\n",
    "\n",
    "        if \"model\" in checkpoint:\n",
    "            self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        if \"objective\" in checkpoint:\n",
    "            self.objective.load_state_dict(checkpoint[\"objective\"])\n",
    "        if \"optimizer\" in checkpoint:\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        if \"epoch\" in checkpoint:\n",
    "            self.epoch = checkpoint[\"epoch\"]\n",
    "        if \"rng_states\" in checkpoint and load_rng:\n",
    "            set_random_states(checkpoint[\"rng_states\"])\n",
    "        if \"scheduler\" in checkpoint and self.scheduler:\n",
    "            self.scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "\n",
    "\n",
    "class ClassifierTrainer:\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        Use config dict to setup BasicTrainer for training classifier.\n",
    "\n",
    "        Config keys:\n",
    "            dataset (dict):\n",
    "                Config dictionary of the training dataset.\n",
    "            backbone (dict):\n",
    "                Config dictionary of the backbone.\n",
    "            objective (dict):\n",
    "                Config dictionary of the objective.\n",
    "            scheduler (dict | None, default: None):\n",
    "                Config dictionary of the scheduler (no scheduler is used by default).\n",
    "            epochs (int):\n",
    "                Number of training epochs.\n",
    "            device (str, default: 'cuda'):\n",
    "                Device to be used.\n",
    "            batch_size (int, default: 128):\n",
    "                Training batch size.\n",
    "            num_workers (int, default: 1):\n",
    "                Number of data loading workers in torch DataLoader.\n",
    "            accumulation_steps (int, default: 1):\n",
    "                Number of gradient accumulation steps.\n",
    "\n",
    "        Returns:\n",
    "            Ready to use BasicTrainer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        config = deepcopy(config)\n",
    "\n",
    "        dataset = realize(\n",
    "            config=config.pop(\"dataset\"),\n",
    "        )\n",
    "        model = realize(\n",
    "            config=config.pop(\"backbone\"),\n",
    "            output_size=dataset.num_classes,\n",
    "        )\n",
    "        objective = realize(\n",
    "            config=config.pop(\"objective\"),\n",
    "        )\n",
    "        optimizer = realize(\n",
    "            config=config.pop(\"optimizer\"),\n",
    "            params=model.parameters(),\n",
    "        )\n",
    "        scheduler = realize(\n",
    "            config=config.pop(\"scheduler\", None),\n",
    "            epochs=config.get(\"epochs\"),\n",
    "        )\n",
    "        epoch_callback = realize(\n",
    "            config=config.pop(\"epoch_callback\", None),\n",
    "        )\n",
    "\n",
    "        return BasicTrainer(\n",
    "            model=model,\n",
    "            dataset=dataset,\n",
    "            objective=objective,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epoch_callback=epoch_callback,\n",
    "            **config,\n",
    "        )\n",
    "\n",
    "\n",
    "class EmbeddingTrainer:\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Use config dict to setup BasicTrainer for training embedder.\n",
    "\n",
    "        Config keys:\n",
    "            dataset (dict):\n",
    "                Config dictionary of the training dataset.\n",
    "            backbone (dict):\n",
    "                Config dictionary of the backbone.\n",
    "            objective (dict):\n",
    "                Config dictionary of the objective.\n",
    "            scheduler (dict | None, default: None):\n",
    "                Config dictionary of the scheduler (no scheduler is used by default).\n",
    "            embedding_size (int | None, default: None):\n",
    "                Adds a linear layer after the backbone with the target embedding size.\n",
    "                By default, embedding size is inferred from backbone (e.g., num_classes=0 in TIMM).\n",
    "            epochs (int):\n",
    "                Number of training epochs.\n",
    "            device (str, default: 'cuda'):\n",
    "                Device to be used.\n",
    "            batch_size (int, default: 128):\n",
    "                Training batch size.\n",
    "            num_workers (int, default: 1):\n",
    "                Number of data loading workers in torch DataLoader.\n",
    "            accumulation_steps (int, default: 1):\n",
    "                Number of gradient accumulation steps.\n",
    "\n",
    "        Returns:\n",
    "            Instance of BasicTrainer\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        config = deepcopy(config)\n",
    "        embedding_size = config.pop(\"embedding_size\", None)\n",
    "\n",
    "        dataset = realize(config=config.pop(\"dataset\"))\n",
    "        backbone = realize(config=config.pop(\"backbone\"), output_size=embedding_size)\n",
    "\n",
    "        if embedding_size is None:  # Infer embedding size\n",
    "            with torch.no_grad():\n",
    "                x = dataset[0][0].unsqueeze(0)\n",
    "                embedding_size = backbone(x).shape[1]\n",
    "\n",
    "        objective = realize(\n",
    "            config=config.pop(\"objective\"),\n",
    "            embedding_size=embedding_size,\n",
    "            num_classes=dataset.num_classes,\n",
    "        )\n",
    "        optimizer = realize(\n",
    "            config=config.pop(\"optimizer\"),\n",
    "            params=chain(backbone.parameters(), objective.parameters()),\n",
    "        )\n",
    "        scheduler = realize(\n",
    "            optimizer=optimizer,\n",
    "            config=config.pop(\"scheduler\", None),\n",
    "            epochs=config.get(\"epochs\"),\n",
    "        )\n",
    "        epoch_callback = realize(\n",
    "            config=config.pop(\"epoch_callback\", None),\n",
    "        )\n",
    "        return BasicTrainer(\n",
    "            model=backbone,\n",
    "            dataset=dataset,\n",
    "            objective=objective,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epoch_callback=epoch_callback,\n",
    "            **config,\n",
    "        )\n",
    "\n",
    "from itertools import chain\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "import wildlife_tools\n",
    "from torch.optim import SGD\n",
    "from wildlife_tools.data import WildlifeDataset\n",
    "from wildlife_tools.train import ArcFaceLoss\n",
    "# , BasicTrainer\n",
    "from importlib import reload\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021v2, LeopardID2022,HyenaID2022\n",
    "\n",
    "# reload(wildlife_tools)\n",
    "# Dataset configuration\n",
    "df = pd.read_csv('/home/wellvw12/full_leopard_4/clients/5/train.csv')\n",
    "metadata = HyenaID2022('/home/wellvw12/leopard')\n",
    "transform = T.Compose([\n",
    "    T.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "    T.RandAugment(num_ops=2, magnitude=20),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "dataset = WildlifeDataset(\n",
    "    metadata = df, \n",
    "    root = metadata.root,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Backbone and loss configuration\n",
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-S-224', num_classes=0, pretrained=True)\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 3, 224, 224)\n",
    "    embedding_size = backbone(dummy_input).shape[1]\n",
    "objective = ArcFaceLoss(num_classes=dataset.num_classes, embedding_size=embedding_size, margin=0.5, scale=64)\n",
    "\n",
    "\n",
    "# Optimizer and scheduler configuration\n",
    "params = chain(backbone.parameters(), objective.parameters())\n",
    "optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n",
    "min_lr = optimizer.defaults.get(\"lr\") * 1e-3\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=min_lr)\n",
    "\n",
    "\n",
    "# Setup training\n",
    "trainer = BasicTrainer(\n",
    "    dataset=dataset,\n",
    "    model=backbone,\n",
    "    objective=objective,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    batch_size=32,\n",
    "    accumulation_steps=2,\n",
    "    num_workers=2,\n",
    "    epochs=100,\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7dac98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
