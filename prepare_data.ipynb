{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import floor\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021v2, LeopardID2022\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, loader, metrics\n",
    "from wildlife_datasets import splits\n",
    "import os\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET HyenaID2022: DOWNLOADING STARTED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyena.coco.tar.gz: 3.44GB [02:57, 19.4MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET HyenaID2022: EXTRACTING STARTED.\n",
      "DATASET HyenaID2022: FINISHED.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HyenaID2022.get_data('~/hyenaid2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test set...\n",
      "\n",
      "Creating KD set...\n",
      "KD set: 350 samples, 35 identities\n",
      "Avg samples/identity: 10.0\n",
      "Remaining: 5770 samples, 394 identities\n",
      "\n",
      "Distributing to clients...\n",
      "\n",
      "Processing clients...\n",
      "\n",
      "Verifying identity mapping for Client 1:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 4\n",
      "Gallery identities: 12\n",
      "Shared identities: 4\n",
      "Client 1 saved - Train: 512, Query: 7, Gallery: 68\n",
      "\n",
      "Verifying identity mapping for Client 2:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 17\n",
      "Gallery identities: 23\n",
      "Shared identities: 17\n",
      "Client 2 saved - Train: 1601, Query: 29, Gallery: 212\n",
      "\n",
      "Verifying identity mapping for Client 3:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 18\n",
      "Gallery identities: 31\n",
      "Shared identities: 18\n",
      "Client 3 saved - Train: 1096, Query: 19, Gallery: 140\n",
      "\n",
      "Verifying identity mapping for Client 4:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 26\n",
      "Gallery identities: 34\n",
      "Shared identities: 26\n",
      "Client 4 saved - Train: 1832, Query: 33, Gallery: 221\n",
      "\n",
      "Final Distribution Summary:\n",
      "Error processing client 0: No columns to parse from file\n",
      "Success: All identities are unique to their clients\n",
      "\n",
      "Client Statistics:\n",
      "Error gathering stats for client 0: No columns to parse from file\n",
      "|   Client | Type   |   Identities |   Train Samples |   Query Samples |   Gallery Samples |   Avg Samples/ID |\n",
      "|---------:|:-------|-------------:|----------------:|----------------:|------------------:|-----------------:|\n",
      "|        1 | Train  |           48 |             512 |               7 |                68 |             10.7 |\n",
      "|        2 | Train  |           85 |            1601 |              29 |               212 |             18.8 |\n",
      "|        3 | Train  |           96 |            1096 |              19 |               140 |             11.4 |\n",
      "|        4 | Train  |          165 |            1832 |              33 |               221 |             11.1 |\n",
      "|        7 | Test   |           36 |             686 |              47 |               639 |             19.1 |\n",
      "\n",
      "KD Set Details:\n",
      "count    35.0\n",
      "mean     10.0\n",
      "std       0.0\n",
      "min      10.0\n",
      "25%      10.0\n",
      "50%      10.0\n",
      "75%      10.0\n",
      "max      10.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from math import floor\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021v2, LeopardID2022, HyenaID2022\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, loader, metrics\n",
    "from wildlife_datasets import splits\n",
    "import os\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_CLIENTS = 4  # 6 clients + 1 test set\n",
    "OUTPUT_DIR = \"/home/wellvw12/lep_4/clients\"\n",
    "DIRICHLET_ALPHA = 0.5\n",
    "MIN_SAMPLES_PER_CLIENT = 90\n",
    "TEST_SIZE = 0.15\n",
    "QUERY_RATIO = 0.2  # Ratio of query samples to gallery samples\n",
    "MIN_GALLERY_PER_QUERY = 2\n",
    "MAX_QUERIES_PER_ID = 8\n",
    "KD_TARGET_SAMPLES = 350  # Total target samples for KD set\n",
    "MIN_KD_SAMPLES_PER_ID = 3\n",
    "MAX_KD_SAMPLES_PER_ID = 10\n",
    "KD_ID_RATIO = 0.3  # Percentage of identities to include in KD set\n",
    "SEPERATE_TEST = 0.085\n",
    "DIRICHLET_ALPHAS = [0.25, 0.4, 0.5, 0.9] \n",
    "\n",
    "def check_identity_mapping(query_set, gallery_set, metadata_path):\n",
    "    \"\"\"Verify consistent identity indexing between query and gallery sets.\"\"\"\n",
    "    metadata = LeopardID2022(metadata_path)\n",
    "    # metadata = HyenaID2022(metadata_path)\n",
    "    q = WildlifeDataset(query_set, metadata.root)\n",
    "    g = WildlifeDataset(gallery_set, metadata.root)\n",
    "    \n",
    "    q_identities = q.labels_map\n",
    "    g_identities = g.labels_map\n",
    "    \n",
    "    # Check 1: Verify all query identities exist in gallery\n",
    "    unique_q = np.unique(q_identities)\n",
    "    unique_g = np.unique(g_identities)\n",
    "    missing = set(unique_q) - set(unique_g)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"Error: {len(missing)} query identities missing from gallery\")\n",
    "        print(\"First 5 missing:\", list(missing)[:5])\n",
    "        return False, {\"missing_identities\": list(missing)}\n",
    "    \n",
    "    # Check 2: Verify index positions match\n",
    "    mismatches = {}\n",
    "    q_indices = {id_: np.where(q_identities == id_)[0] for id_ in unique_q}\n",
    "    g_indices = {id_: np.where(g_identities == id_)[0] for id_ in unique_g}\n",
    "    \n",
    "    for id_ in set(unique_q) & set(unique_g):\n",
    "        if not np.array_equal(q_indices[id_], g_indices[id_]):\n",
    "            mismatches[id_] = {\n",
    "                \"query_indices\": q_indices[id_].tolist(),\n",
    "                \"gallery_indices\": g_indices[id_].tolist()\n",
    "            }\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"Error: {len(mismatches)} identities have index mismatches\")\n",
    "        for id_, idx in list(mismatches.items())[:3]:\n",
    "            print(f\"{id_}:\\n  Query positions: {idx['query_indices']}\\n  Gallery positions: {idx['gallery_indices']}\")\n",
    "        return False, {\"index_mismatches\": mismatches}\n",
    "    \n",
    "    # Check 3: Verify no duplicates\n",
    "    q_dupes = [id_ for id_ in unique_q if len(q_indices[id_]) > 1]\n",
    "    g_dupes = [id_ for id_ in unique_g if len(g_indices[id_]) > 1]\n",
    "    \n",
    "    if q_dupes or g_dupes:\n",
    "        print(\"Error: Duplicate identity indices found\")\n",
    "        if q_dupes: print(f\"Query duplicates ({len(q_dupes)}):\", q_dupes[:5])\n",
    "        if g_dupes: print(f\"Gallery duplicates ({len(g_dupes)}):\", g_dupes[:5])\n",
    "        return False, {\"query_duplicates\": q_dupes, \"gallery_duplicates\": g_dupes}\n",
    "    \n",
    "    print(\"Success: Identity mappings are consistent\")\n",
    "    print(f\"Query identities: {len(unique_q)}\")\n",
    "    print(f\"Gallery identities: {len(unique_g)}\")\n",
    "    print(f\"Shared identities: {len(set(unique_q) & set(unique_g))}\")\n",
    "    \n",
    "    return True, {\n",
    "        \"query_identities\": len(unique_q),\n",
    "        \"gallery_identities\": len(unique_g),\n",
    "        \"shared_identities\": len(set(unique_q) & set(unique_g))\n",
    "    }\n",
    "\n",
    "def create_query_gallery_splits(\n",
    "    df, \n",
    "    max_queries, \n",
    "    min_gallery, \n",
    "    min_samples, \n",
    "    query_ratio=QUERY_RATIO, \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Create query/gallery splits with guaranteed gallery support.\n",
    "    Tries to ensure queries make up about `query_ratio` of gallery samples.\n",
    "    \"\"\"\n",
    "    results = {'query': [], 'gallery': []}\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    for identity, count in id_counts.items():\n",
    "        samples = df[df['identity'] == identity]\n",
    "        possible_queries = min(\n",
    "            max_queries, \n",
    "            max(1, int(count * query_ratio))\n",
    "        )\n",
    "        if count >= min_samples and possible_queries > 0 and (count - possible_queries) >= min_gallery:\n",
    "            query_samples = samples.sample(possible_queries, random_state=random_state)\n",
    "            results['query'].append(query_samples)\n",
    "            results['gallery'].append(samples.drop(query_samples.index))\n",
    "        else:\n",
    "            results['gallery'].append(samples)\n",
    "    query_df = pd.concat(results['query']) if results['query'] else pd.DataFrame()\n",
    "    gallery_df = pd.concat(results['gallery'])\n",
    "    gallery_df = gallery_df[~gallery_df.index.isin(query_df.index)]\n",
    "    query_df = query_df[query_df['identity'].isin(gallery_df['identity'])]\n",
    "    return query_df, gallery_df\n",
    "\n",
    "def process_test_set(test_set_dir, metadata_path='/home/wellvw12/leopard'):\n",
    "    \"\"\"\n",
    "    Process an individual test set directory to create validated query/gallery splits.\n",
    "    \n",
    "    Args:\n",
    "        test_set_dir: Path to directory containing test set CSV\n",
    "        metadata_path: Path to dataset root for WildlifeDataset initialization\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_status, report_dict)\n",
    "    \"\"\"\n",
    "    # Path setup\n",
    "    test_csv_path = os.path.join(test_set_dir, 'test.csv')\n",
    "    output_query_path = os.path.join(test_set_dir, 'query.csv')\n",
    "    output_gallery_path = os.path.join(test_set_dir, 'gallery.csv')\n",
    "    \n",
    "    if not os.path.exists(test_csv_path):\n",
    "        print(f\"Error: test.csv not found in {test_set_dir}\")\n",
    "        return False, {\"error\": \"test.csv not found\"}\n",
    "    \n",
    "    try:\n",
    "        # Load test set\n",
    "        test_df = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        # Create query/gallery splits\n",
    "        query_df, gallery_df = create_query_gallery_splits(test_df)\n",
    "        \n",
    "        # Verify identity mappings\n",
    "        print(f\"\\nVerifying identity mappings for {test_set_dir}:\")\n",
    "        is_consistent, verification_report = check_identity_mapping(\n",
    "            query_df, gallery_df, metadata_path\n",
    "        )\n",
    "        \n",
    "        if not is_consistent:\n",
    "            print(\"Aborting due to identity mapping issues\")\n",
    "            return False, verification_report\n",
    "        \n",
    "        # Save the validated splits\n",
    "        query_df.to_csv(output_query_path, index=False)\n",
    "        gallery_df.to_csv(output_gallery_path, index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nSuccessfully processed {test_set_dir}:\")\n",
    "        print(f\"  Query samples: {len(query_df)}\")\n",
    "        print(f\"  Gallery samples: {len(gallery_df)}\")\n",
    "        print(f\"  Query identities: {len(query_df['identity'].unique())}\")\n",
    "        print(f\"  Gallery identities: {len(gallery_df['identity'].unique())}\")\n",
    "        \n",
    "        return True, {\n",
    "            \"query_samples\": len(query_df),\n",
    "            \"gallery_samples\": len(gallery_df),\n",
    "            \"query_identities\": len(query_df['identity'].unique()),\n",
    "            \"gallery_identities\": len(gallery_df['identity'].unique()),\n",
    "            \"verification_report\": verification_report\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {test_set_dir}: {str(e)}\")\n",
    "        return False, {\"error\": str(e)}\n",
    "\n",
    "# Load dataset\n",
    "d = LeopardID2022('/home/wellvw12/leopard')\n",
    "# d = HyenaID2022('/home/wellvw12/hyenaid2022')\n",
    "df = d.df[~(d.df['identity'] == 'unknown')]\n",
    "\n",
    "# Parameters\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def save_dataframe(df, path):\n",
    "    \"\"\"Safely save DataFrame, creating empty file if DataFrame is empty\"\"\"\n",
    "    if len(df) == 0:\n",
    "        df.head(0).to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "# --- Step 1: First split into Test and Non-Test ---\n",
    "print(\"Creating test set...\")\n",
    "test_identities = np.random.choice(\n",
    "    df['identity'].unique(), \n",
    "    size=int(len(df['identity'].unique()) * SEPERATE_TEST),\n",
    "    replace=False\n",
    ")\n",
    "test_df = df[df['identity'].isin(test_identities)]\n",
    "non_test_df = df[~df['identity'].isin(test_identities)]\n",
    "\n",
    "# --- Step 2: Create Balanced KD Set ---\n",
    "print(\"\\nCreating KD set...\")\n",
    "def create_balanced_kd_set(df, target_samples=KD_TARGET_SAMPLES, \n",
    "                          min_samples=MIN_KD_SAMPLES_PER_ID, \n",
    "                          max_samples=MAX_KD_SAMPLES_PER_ID,\n",
    "                          id_ratio=KD_ID_RATIO):\n",
    "    # Select identities for KD set (prioritize those with more samples)\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    kd_candidate_ids = id_counts[id_counts >= min_samples].index\n",
    "    n_kd_ids = max(int(len(id_counts) * id_ratio), 30)  # At least 30 identities\n",
    "    \n",
    "    # Sort by sample count and take top identities that give us target_samples\n",
    "    sorted_ids = id_counts.sort_values(ascending=False).index\n",
    "    selected_ids = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    for identity in sorted_ids:\n",
    "        if identity in kd_candidate_ids and len(selected_ids) < n_kd_ids:\n",
    "            group = df[df['identity'] == identity]\n",
    "            samples = min(max_samples, len(group))\n",
    "            if total_samples + samples <= target_samples or len(selected_ids) < 30:\n",
    "                selected_ids.append(identity)\n",
    "                total_samples += samples\n",
    "    \n",
    "    # Now sample from selected identities\n",
    "    kd_samples = []\n",
    "    remaining_samples = []\n",
    "    \n",
    "    for identity in df['identity'].unique():\n",
    "        group = df[df['identity'] == identity]\n",
    "        if identity in selected_ids:\n",
    "            n_samples = min(max_samples, max(min_samples, len(group)))\n",
    "            kd_samples.append(group.sample(n=n_samples, random_state=42))\n",
    "            remaining_samples.append(group.drop(kd_samples[-1].index))\n",
    "        else:\n",
    "            remaining_samples.append(group)\n",
    "    \n",
    "    kd_df = pd.concat(kd_samples)\n",
    "    remaining_df = pd.concat(remaining_samples)\n",
    "    \n",
    "    print(f\"KD set: {len(kd_df)} samples, {kd_df['identity'].nunique()} identities\")\n",
    "    print(f\"Avg samples/identity: {len(kd_df)/kd_df['identity'].nunique():.1f}\")\n",
    "    print(f\"Remaining: {len(remaining_df)} samples, {remaining_df['identity'].nunique()} identities\")\n",
    "    return kd_df, remaining_df\n",
    "\n",
    "kd_df, client_df = create_balanced_kd_set(non_test_df)\n",
    "\n",
    "# Save KD set as \"client 0\"\n",
    "kd_dir = f\"{OUTPUT_DIR}/0\"\n",
    "os.makedirs(kd_dir, exist_ok=True)\n",
    "save_dataframe(kd_df, f\"{kd_dir}/train.csv\")\n",
    "save_dataframe(pd.DataFrame(), f\"{kd_dir}/query.csv\")\n",
    "save_dataframe(pd.DataFrame(), f\"{kd_dir}/gallery.csv\")\n",
    "\n",
    "# --- Step 3: Distribute Remaining to Clients ---\n",
    "print(\"\\nDistributing to clients...\")\n",
    "all_identities = client_df['identity'].unique()\n",
    "np.random.seed(42)\n",
    "client_assignments = {i: [] for i in range(1, N_CLIENTS+1)}  # Clients 1-6\n",
    "\n",
    "# ...existing code...\n",
    " # Example: 5 clients, first two are \"smaller\"\n",
    "probs = np.random.dirichlet(DIRICHLET_ALPHAS, size=len(all_identities))\n",
    "\n",
    "client_assignments = {i: [] for i in range(1, N_CLIENTS+1)}\n",
    "for idx, identity in enumerate(all_identities):\n",
    "    client_id = np.random.choice(N_CLIENTS, p=probs[idx]) + 1\n",
    "    client_assignments[client_id].append(identity)\n",
    "# ...existing code...\n",
    "\n",
    "# Rebalance clients\n",
    "for client_id in client_assignments:\n",
    "    client_data = client_df[client_df['identity'].isin(client_assignments[client_id])]\n",
    "    while len(client_data) < MIN_SAMPLES_PER_CLIENT:\n",
    "        richest_client = max(client_assignments.items(), \n",
    "                           key=lambda x: len(client_df[client_df['identity'].isin(x[1])]))[0]\n",
    "        transfer_ids = client_assignments[richest_client][-1:]  # Transfer one identity\n",
    "        client_assignments[richest_client] = client_assignments[richest_client][:-1]\n",
    "        client_assignments[client_id].extend(transfer_ids)\n",
    "        client_data = client_df[client_df['identity'].isin(client_assignments[client_id])]\n",
    "\n",
    "# --- Step 4: Save Test Set as Client 7 ---\n",
    "test_dir = f\"{OUTPUT_DIR}/7\"\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "save_dataframe(test_df, f\"{test_dir}/train.csv\")\n",
    "\n",
    "# Create test query/gallery splits\n",
    "test_query, test_gallery = create_query_gallery_splits(\n",
    "    test_df,\n",
    "    max_queries=MAX_QUERIES_PER_ID,\n",
    "    min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "    min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "    query_ratio=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "save_dataframe(test_query, f\"{test_dir}/query.csv\")\n",
    "save_dataframe(test_gallery, f\"{test_dir}/gallery.csv\")\n",
    "\n",
    "# --- Step 5: Process Each Client ---\n",
    "print(\"\\nProcessing clients...\")\n",
    "used_identities = set()\n",
    "for client_id, identities in client_assignments.items():\n",
    "    client_dir = f\"{OUTPUT_DIR}/{client_id}\"\n",
    "    os.makedirs(client_dir, exist_ok=True)\n",
    "    identities = [id_ for id_ in identities if id_ not in used_identities]\n",
    "    used_identities.update(identities)\n",
    "    client_data = client_df[client_df['identity'].isin(identities)]\n",
    "    \n",
    "    # Split into train/test (15% test)\n",
    "    id_counts = client_data['identity'].value_counts()\n",
    "    multi_sample_ids = id_counts[id_counts > 1].index\n",
    "    \n",
    "    if len(multi_sample_ids) == 0:\n",
    "        train = client_data\n",
    "        test = pd.DataFrame(columns=client_data.columns)\n",
    "    else:\n",
    "        df_multi = client_data[client_data['identity'].isin(multi_sample_ids)]\n",
    "        df_single = client_data[~client_data['identity'].isin(multi_sample_ids)]\n",
    "        \n",
    "        if len(df_multi) < 2:\n",
    "            train = client_data\n",
    "            test = pd.DataFrame(columns=client_data.columns)\n",
    "        else:\n",
    "            try:\n",
    "                train_multi, test_multi = train_test_split(\n",
    "                    df_multi, test_size=TEST_SIZE, \n",
    "                    stratify=df_multi['identity'], random_state=42\n",
    "                )\n",
    "            except ValueError:\n",
    "                train_multi, test_multi = train_test_split(\n",
    "                    df_multi, test_size=TEST_SIZE, random_state=42\n",
    "                )\n",
    "            \n",
    "            train = pd.concat([train_multi, df_single])\n",
    "            test = test_multi\n",
    "    \n",
    "    # Create query/gallery from test set\n",
    "    if len(test) > 0:\n",
    "        query, gallery = create_query_gallery_splits(\n",
    "            test,\n",
    "            max_queries=MAX_QUERIES_PER_ID,\n",
    "            min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "            min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "            query_ratio=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        query = query[query['identity'].isin(gallery['identity'])]\n",
    "        test_id_counts = test['identity'].value_counts()\n",
    "        problematic = test_id_counts[test_id_counts == 1].index\n",
    "        if len(problematic) > 0:\n",
    "            train = pd.concat([train, test[test['identity'].isin(problematic)]])\n",
    "            query = query[~query['identity'].isin(problematic)]\n",
    "            gallery = gallery[~gallery['identity'].isin(problematic)]\n",
    "        print(f\"\\nVerifying identity mapping for Client {client_id}:\")\n",
    "        is_consistent, report = check_identity_mapping(\n",
    "            query, gallery, '/home/wellvw12/leopard'\n",
    "        )\n",
    "    else:\n",
    "        query = pd.DataFrame(columns=client_data.columns)\n",
    "        gallery = pd.DataFrame(columns=client_data.columns)\n",
    "    save_dataframe(train, f\"{client_dir}/train.csv\")\n",
    "    save_dataframe(query, f\"{client_dir}/query.csv\")\n",
    "    save_dataframe(gallery, f\"{client_dir}/gallery.csv\")\n",
    "    print(f\"Client {client_id} saved - Train: {len(train)}, Query: {len(query)}, Gallery: {len(gallery)}\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nFinal Distribution Summary:\")\n",
    "all_identities = defaultdict(list)\n",
    "\n",
    "for client in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if not client.isdigit():\n",
    "        continue\n",
    "        \n",
    "    client_path = f\"{OUTPUT_DIR}/{client}\"\n",
    "    try:\n",
    "        train = pd.read_csv(f\"{client_path}/train.csv\")\n",
    "        query = pd.read_csv(f\"{client_path}/query.csv\") if os.path.getsize(f\"{client_path}/query.csv\") > 0 else pd.DataFrame()\n",
    "        gallery = pd.read_csv(f\"{client_path}/gallery.csv\") if os.path.getsize(f\"{client_path}/gallery.csv\") > 0 else pd.DataFrame()\n",
    "        \n",
    "        identities = train['identity'].unique() if 'identity' in train.columns else []\n",
    "        \n",
    "        for id_ in identities:\n",
    "            all_identities[id_].append(client)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing client {client}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Check for overlapping identities\n",
    "overlaps = {k:v for k,v in all_identities.items() if len(v) > 1}\n",
    "if overlaps:\n",
    "    print(f\"Warning: {len(overlaps)} identities shared across clients\")\n",
    "    print(\"Sample overlaps:\", dict(list(overlaps.items())[:3]))\n",
    "else:\n",
    "    print(\"Success: All identities are unique to their clients\")\n",
    "\n",
    "print(\"\\nClient Statistics:\")\n",
    "stats = []\n",
    "for client in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if not client.isdigit():\n",
    "        continue\n",
    "        \n",
    "    client_path = f\"{OUTPUT_DIR}/{client}\"\n",
    "    try:\n",
    "        train = pd.read_csv(f\"{client_path}/train.csv\")\n",
    "        query = pd.read_csv(f\"{client_path}/query.csv\") if os.path.getsize(f\"{client_path}/query.csv\") > 0 else pd.DataFrame()\n",
    "        gallery = pd.read_csv(f\"{client_path}/gallery.csv\") if os.path.getsize(f\"{client_path}/gallery.csv\") > 0 else pd.DataFrame()\n",
    "        \n",
    "        stats.append({\n",
    "            'Client': client,\n",
    "            'Type': 'KD' if client == '0' else 'Test' if client == '7' else 'Train',\n",
    "            'Identities': train['identity'].nunique() if 'identity' in train.columns else 0,\n",
    "            'Train Samples': len(train),\n",
    "            'Query Samples': len(query),\n",
    "            'Gallery Samples': len(gallery),\n",
    "            'Avg Samples/ID': round(len(train)/train['identity'].nunique(), 1) if 'identity' in train.columns and train['identity'].nunique() > 0 else 0\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error gathering stats for client {client}: {str(e)}\")\n",
    "\n",
    "print(pd.DataFrame(stats).to_markdown(index=False))\n",
    "print(\"\\nKD Set Details:\")\n",
    "print(kd_df['identity'].value_counts().describe())\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from wildlife_datasets.datasets import HyenaID2022\n",
    "\n",
    "# Global parameters\n",
    "N_CLIENTS = 4\n",
    "OUTPUT_DIR = \"/home/wellvw12/hyena_6/clients\"\n",
    "DIRICHLET_ALPHA = 0.5\n",
    "MIN_SAMPLES_PER_CLIENT = 90\n",
    "TEST_SIZE = 0.25\n",
    "QUERY_RATIO = 0.2\n",
    "MIN_GALLERY_PER_QUERY = 2\n",
    "MAX_QUERIES_PER_ID = 8\n",
    "SEPERATE_TEST = 0.085\n",
    "DIRICHLET_ALPHAS = [0.45, 0.45, 0.6, 0.9]\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "def create_query_gallery_splits(\n",
    "    df, \n",
    "    max_queries=MAX_QUERIES_PER_ID, \n",
    "    min_gallery=MIN_GALLERY_PER_QUERY, \n",
    "    min_samples=MIN_GALLERY_PER_QUERY+1, \n",
    "    query_ratio=QUERY_RATIO, \n",
    "    random_state=RANDOM_SEED\n",
    "):\n",
    "    \"\"\"\n",
    "    Create query/gallery splits ensuring query images are in gallery.\n",
    "    \"\"\"\n",
    "    results = {'query': [], 'gallery': []}\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    \n",
    "    for identity, count in id_counts.items():\n",
    "        samples = df[df['identity'] == identity]\n",
    "        \n",
    "        # Calculate number of query samples\n",
    "        possible_queries = min(\n",
    "            max_queries, \n",
    "            max(1, int(count * query_ratio))\n",
    "        )\n",
    "        \n",
    "        # Only create queries if we have enough samples for gallery support\n",
    "        if count >= min_samples and possible_queries > 0 and (count - possible_queries) >= min_gallery:\n",
    "            # Sample query images\n",
    "            query_samples = samples.sample(possible_queries, random_state=random_state)\n",
    "            results['query'].append(query_samples)\n",
    "            \n",
    "            # Gallery includes ALL samples (including query images)\n",
    "            results['gallery'].append(samples)\n",
    "        else:\n",
    "            # If not enough samples for queries, all go to gallery\n",
    "            results['gallery'].append(samples)\n",
    "    \n",
    "    query_df = pd.concat(results['query']) if results['query'] else pd.DataFrame()\n",
    "    gallery_df = pd.concat(results['gallery'])\n",
    "    \n",
    "    # Ensure query images are present in gallery\n",
    "    if len(query_df) > 0:\n",
    "        query_df = query_df[query_df['identity'].isin(gallery_df['identity'])]\n",
    "    \n",
    "    return query_df, gallery_df\n",
    "\n",
    "def create_orientation_based_distribution(df, n_clients=N_CLIENTS):\n",
    "    \"\"\"\n",
    "    Create non-IID distribution based on orientation with realistic heterogeneity.\n",
    "    \"\"\"\n",
    "    print(\"Creating orientation-based non-IID distribution...\")\n",
    "    \n",
    "    # Check if orientation column exists\n",
    "    if 'orientation' not in df.columns:\n",
    "        print(\"Error: No 'orientation' column found in dataset\")\n",
    "        return None\n",
    "    \n",
    "    # Get orientation distribution\n",
    "    orientation_counts = df['orientation'].value_counts()\n",
    "    print(\"Orientation distribution:\")\n",
    "    print(orientation_counts)\n",
    "    \n",
    "    # Define client-specific orientation distributions for strong non-IID\n",
    "    client_distributions = {\n",
    "        1: {  # Right-dominant client\n",
    "            'right': 1000,\n",
    "            'left': 200,\n",
    "            'frontright': 50,\n",
    "            'backright': 50\n",
    "        },\n",
    "        2: {  # Left-dominant client\n",
    "            'left': 1000,\n",
    "            'right': 200,\n",
    "            'frontleft': 80,\n",
    "            'backleft': 40\n",
    "        },\n",
    "        3: {  # Front-focused client\n",
    "            'frontleft': 51,\n",
    "            'frontright': 33,\n",
    "            'front': 51,\n",
    "            'right': 142,\n",
    "            'left': 94\n",
    "        },\n",
    "        4: {  # Rare orientations client\n",
    "            'up': 50,\n",
    "            'back': 20,\n",
    "            'down': 10,\n",
    "            'backleft': 27,\n",
    "            'backright': 31\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    client_assignments = {i: [] for i in range(1, n_clients+1)}\n",
    "    assigned_samples = set()\n",
    "    \n",
    "    # Assign samples based on orientation preferences\n",
    "    for client_id, preferences in client_distributions.items():\n",
    "        if client_id > n_clients:\n",
    "            continue\n",
    "            \n",
    "        client_samples = []\n",
    "        \n",
    "        for orientation, target_count in preferences.items():\n",
    "            if orientation in df['orientation'].values:\n",
    "                # Get available samples for this orientation\n",
    "                orientation_samples = df[\n",
    "                    (df['orientation'] == orientation) & \n",
    "                    (~df.index.isin(assigned_samples))\n",
    "                ]\n",
    "                \n",
    "                # Sample up to target_count or all available\n",
    "                n_samples = min(target_count, len(orientation_samples))\n",
    "                if n_samples > 0:\n",
    "                    sampled = orientation_samples.sample(n=n_samples, random_state=RANDOM_SEED)\n",
    "                    client_samples.append(sampled)\n",
    "                    assigned_samples.update(sampled.index)\n",
    "        \n",
    "        if client_samples:\n",
    "            client_data = pd.concat(client_samples)\n",
    "            client_assignments[client_id] = client_data.index.tolist()\n",
    "            print(f\"Client {client_id}: {len(client_data)} samples\")\n",
    "            print(f\"  Orientations: {client_data['orientation'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Handle remaining unassigned samples\n",
    "    remaining_samples = df[~df.index.isin(assigned_samples)]\n",
    "    if len(remaining_samples) > 0:\n",
    "        print(f\"Distributing {len(remaining_samples)} remaining samples...\")\n",
    "        \n",
    "        # Find client with fewest samples\n",
    "        client_sizes = {i: len(client_assignments[i]) for i in range(1, n_clients+1)}\n",
    "        smallest_client = min(client_sizes, key=client_sizes.get)\n",
    "        \n",
    "        client_assignments[smallest_client].extend(remaining_samples.index.tolist())\n",
    "        print(f\"Added {len(remaining_samples)} samples to Client {smallest_client}\")\n",
    "    \n",
    "    return client_assignments\n",
    "\n",
    "def save_dataframe(df, path):\n",
    "    \"\"\"Safely save DataFrame, creating empty file if DataFrame is empty\"\"\"\n",
    "    if len(df) == 0:\n",
    "        df.head(0).to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading HyenaID2022 dataset...\")\n",
    "d = HyenaID2022('/home/wellvw12/hyenaid2022')\n",
    "df = d.df[~(d.df['identity'] == 'unknown')]\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total identities: {df['identity'].nunique()}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Step 1: Create global test set\n",
    "print(\"\\nCreating global test set...\")\n",
    "test_identities = np.random.choice(\n",
    "    df['identity'].unique(), \n",
    "    size=int(len(df['identity'].unique()) * SEPERATE_TEST),\n",
    "    replace=False\n",
    ")\n",
    "test_df = df[df['identity'].isin(test_identities)]\n",
    "client_df = df[~df['identity'].isin(test_identities)]\n",
    "\n",
    "print(f\"Test set: {len(test_df)} samples, {test_df['identity'].nunique()} identities\")\n",
    "print(f\"Client data: {len(client_df)} samples, {client_df['identity'].nunique()} identities\")\n",
    "\n",
    "# Step 2: Create orientation-based client assignments\n",
    "client_assignments = create_orientation_based_distribution(client_df, N_CLIENTS)\n",
    "\n",
    "if client_assignments is None:\n",
    "    print(\"Error: Could not create orientation-based distribution\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 3: Create and save client data\n",
    "print(\"\\nCreating client datasets...\")\n",
    "for client_id in range(1, N_CLIENTS+1):\n",
    "    client_dir = f\"{OUTPUT_DIR}/{client_id}\"\n",
    "    os.makedirs(client_dir, exist_ok=True)\n",
    "    \n",
    "    # Get client data\n",
    "    client_indices = client_assignments[client_id]\n",
    "    client_data = client_df.loc[client_indices]\n",
    "    \n",
    "    # Create query/gallery splits\n",
    "    query_df, gallery_df = create_query_gallery_splits(\n",
    "        client_data,\n",
    "        max_queries=MAX_QUERIES_PER_ID,\n",
    "        min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "        min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "        query_ratio=QUERY_RATIO,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Save client data\n",
    "    save_dataframe(client_data, f\"{client_dir}/train.csv\")\n",
    "    save_dataframe(query_df, f\"{client_dir}/query.csv\")\n",
    "    save_dataframe(gallery_df, f\"{client_dir}/gallery.csv\")\n",
    "    \n",
    "    print(f\"Client {client_id} saved:\")\n",
    "    print(f\"  Train: {len(client_data)} samples, {client_data['identity'].nunique()} identities\")\n",
    "    print(f\"  Query: {len(query_df)} samples\")\n",
    "    print(f\"  Gallery: {len(gallery_df)} samples\")\n",
    "    if len(client_data) > 0:\n",
    "        print(f\"  Top orientations: {client_data['orientation'].value_counts().head(3).to_dict()}\")\n",
    "\n",
    "# Step 4: Create global test set with query/gallery splits\n",
    "print(\"\\nCreating global test set...\")\n",
    "test_dir = f\"{OUTPUT_DIR}/test\"\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "test_query_df, test_gallery_df = create_query_gallery_splits(\n",
    "    test_df,\n",
    "    max_queries=MAX_QUERIES_PER_ID,\n",
    "    min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "    min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "    query_ratio=0.3,  # Higher ratio for test set\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "save_dataframe(test_df, f\"{test_dir}/all.csv\")\n",
    "save_dataframe(test_query_df, f\"{test_dir}/query.csv\")\n",
    "save_dataframe(test_gallery_df, f\"{test_dir}/gallery.csv\")\n",
    "\n",
    "print(f\"Global test set saved:\")\n",
    "print(f\"  All: {len(test_df)} samples, {test_df['identity'].nunique()} identities\")\n",
    "print(f\"  Query: {len(test_query_df)} samples\")\n",
    "print(f\"  Gallery: {len(test_gallery_df)} samples\")\n",
    "\n",
    "# Step 5: Verification\n",
    "print(\"\\nVerification:\")\n",
    "\n",
    "# Check for identity overlap between clients\n",
    "all_client_identities = set()\n",
    "identity_overlaps = []\n",
    "\n",
    "for client_id in range(1, N_CLIENTS+1):\n",
    "    client_path = f\"{OUTPUT_DIR}/{client_id}/train.csv\"\n",
    "    if os.path.exists(client_path):\n",
    "        client_train = pd.read_csv(client_path)\n",
    "        client_identities = set(client_train['identity'].unique())\n",
    "        \n",
    "        # Check for overlap with previous clients\n",
    "        overlap = all_client_identities.intersection(client_identities)\n",
    "        if overlap:\n",
    "            identity_overlaps.append(f\"Client {client_id} overlaps: {len(overlap)} identities\")\n",
    "        \n",
    "        all_client_identities.update(client_identities)\n",
    "\n",
    "if identity_overlaps:\n",
    "    print(\"Warning: Identity overlaps detected:\")\n",
    "    for overlap in identity_overlaps:\n",
    "        print(f\"  {overlap}\")\n",
    "else:\n",
    "    print(\"✓ No identity overlaps between clients\")\n",
    "\n",
    "# Verify query images are in gallery\n",
    "print(\"\\nVerifying query images are in gallery:\")\n",
    "for client_id in range(1, N_CLIENTS+1):\n",
    "    query_path = f\"{OUTPUT_DIR}/{client_id}/query.csv\"\n",
    "    gallery_path = f\"{OUTPUT_DIR}/{client_id}/gallery.csv\"\n",
    "    \n",
    "    if os.path.exists(query_path) and os.path.exists(gallery_path):\n",
    "        query_df = pd.read_csv(query_path)\n",
    "        gallery_df = pd.read_csv(gallery_path)\n",
    "        \n",
    "        if len(query_df) > 0 and len(gallery_df) > 0:\n",
    "            query_images = set(query_df['image_id'] if 'image_id' in query_df.columns else query_df.index)\n",
    "            gallery_images = set(gallery_df['image_id'] if 'image_id' in gallery_df.columns else gallery_df.index)\n",
    "            \n",
    "            missing = query_images - gallery_images\n",
    "            if missing:\n",
    "                print(f\"  Client {client_id}: {len(missing)} query images missing from gallery\")\n",
    "            else:\n",
    "                print(f\"  Client {client_id}: ✓ All query images in gallery\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Clients created: {N_CLIENTS}\")\n",
    "print(f\"Global test set created: {len(test_df)} samples\")\n",
    "print(f\"Random seed used: {RANDOM_SEED}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
