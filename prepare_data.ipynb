{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import floor\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021v2, LeopardID2022, HyenaID2022\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, loader, metrics\n",
    "from wildlife_datasets import splits\n",
    "import os\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET HyenaID2022: DOWNLOADING STARTED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hyena.coco.tar.gz: 3.44GB [02:57, 19.4MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET HyenaID2022: EXTRACTING STARTED.\n",
      "DATASET HyenaID2022: FINISHED.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HyenaID2022.get_data('~/hyenaid2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test set...\n",
      "\n",
      "Creating KD set...\n",
      "KD set: 600 samples, 60 identities\n",
      "Avg samples/identity: 10.0\n",
      "Remaining: 5875 samples, 394 identities\n",
      "\n",
      "Distributing to clients...\n",
      "\n",
      "Processing clients...\n",
      "\n",
      "Verifying identity mapping for Client 1:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 5\n",
      "Gallery identities: 10\n",
      "Shared identities: 5\n",
      "Client 1 saved - Train: 248, Query: 6, Gallery: 52\n",
      "\n",
      "Verifying identity mapping for Client 2:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 12\n",
      "Gallery identities: 17\n",
      "Shared identities: 12\n",
      "Client 2 saved - Train: 577, Query: 22, Gallery: 150\n",
      "\n",
      "Verifying identity mapping for Client 3:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 24\n",
      "Gallery identities: 36\n",
      "Shared identities: 24\n",
      "Client 3 saved - Train: 1155, Query: 38, Gallery: 297\n",
      "\n",
      "Verifying identity mapping for Client 4:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 16\n",
      "Gallery identities: 23\n",
      "Shared identities: 16\n",
      "Client 4 saved - Train: 1011, Query: 32, Gallery: 271\n",
      "\n",
      "Verifying identity mapping for Client 5:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 34\n",
      "Gallery identities: 59\n",
      "Shared identities: 34\n",
      "Client 5 saved - Train: 1564, Query: 50, Gallery: 402\n",
      "\n",
      "Final Distribution Summary:\n",
      "Error processing client 0: No columns to parse from file\n",
      "Success: All identities are unique to their clients\n",
      "\n",
      "Client Statistics:\n",
      "Error gathering stats for client 0: No columns to parse from file\n",
      "|   Client | Type   |   Identities |   Train Samples |   Query Samples |   Gallery Samples |   Avg Samples/ID |\n",
      "|---------:|:-------|-------------:|----------------:|----------------:|------------------:|-----------------:|\n",
      "|        1 | Train  |           32 |             248 |               6 |                52 |              7.8 |\n",
      "|        2 | Train  |           46 |             577 |              22 |               150 |             12.5 |\n",
      "|        3 | Train  |           96 |            1155 |              38 |               297 |             12   |\n",
      "|        4 | Train  |           67 |            1011 |              32 |               271 |             15.1 |\n",
      "|        5 | Train  |          153 |            1564 |              50 |               402 |             10.2 |\n",
      "|        7 | Test   |           36 |             331 |              39 |               292 |              9.2 |\n",
      "\n",
      "KD Set Details:\n",
      "count    60.0\n",
      "mean     10.0\n",
      "std       0.0\n",
      "min      10.0\n",
      "25%      10.0\n",
      "50%      10.0\n",
      "75%      10.0\n",
      "max      10.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from math import floor\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021v2, LeopardID2022\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, loader, metrics\n",
    "from wildlife_datasets import splits\n",
    "import os\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_CLIENTS = 5  # 6 clients + 1 test set\n",
    "OUTPUT_DIR = \"/home/wellvw12/full_leopard_4/clients\"\n",
    "DIRICHLET_ALPHA = 0.5\n",
    "MIN_SAMPLES_PER_CLIENT = 90\n",
    "TEST_SIZE = 0.25\n",
    "QUERY_RATIO = 0.2  # Ratio of query samples to gallery samples\n",
    "MIN_GALLERY_PER_QUERY = 2\n",
    "MAX_QUERIES_PER_ID = 8\n",
    "KD_TARGET_SAMPLES = 600  # Total target samples for KD set\n",
    "MIN_KD_SAMPLES_PER_ID = 3\n",
    "MAX_KD_SAMPLES_PER_ID = 10\n",
    "KD_ID_RATIO = 0.3  # Percentage of identities to include in KD set\n",
    "SEPERATE_TEST = 0.085\n",
    "DIRICHLET_ALPHAS = [0.25, 0.25, 0.5, 0.5, 0.9] \n",
    "\n",
    "def check_identity_mapping(query_set, gallery_set, metadata_path):\n",
    "    \"\"\"Verify consistent identity indexing between query and gallery sets.\"\"\"\n",
    "    metadata = LeopardID2022(metadata_path)\n",
    "    q = WildlifeDataset(query_set, metadata.root)\n",
    "    g = WildlifeDataset(gallery_set, metadata.root)\n",
    "    \n",
    "    q_identities = q.labels_map\n",
    "    g_identities = g.labels_map\n",
    "    \n",
    "    # Check 1: Verify all query identities exist in gallery\n",
    "    unique_q = np.unique(q_identities)\n",
    "    unique_g = np.unique(g_identities)\n",
    "    missing = set(unique_q) - set(unique_g)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"Error: {len(missing)} query identities missing from gallery\")\n",
    "        print(\"First 5 missing:\", list(missing)[:5])\n",
    "        return False, {\"missing_identities\": list(missing)}\n",
    "    \n",
    "    # Check 2: Verify index positions match\n",
    "    mismatches = {}\n",
    "    q_indices = {id_: np.where(q_identities == id_)[0] for id_ in unique_q}\n",
    "    g_indices = {id_: np.where(g_identities == id_)[0] for id_ in unique_g}\n",
    "    \n",
    "    for id_ in set(unique_q) & set(unique_g):\n",
    "        if not np.array_equal(q_indices[id_], g_indices[id_]):\n",
    "            mismatches[id_] = {\n",
    "                \"query_indices\": q_indices[id_].tolist(),\n",
    "                \"gallery_indices\": g_indices[id_].tolist()\n",
    "            }\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"Error: {len(mismatches)} identities have index mismatches\")\n",
    "        for id_, idx in list(mismatches.items())[:3]:\n",
    "            print(f\"{id_}:\\n  Query positions: {idx['query_indices']}\\n  Gallery positions: {idx['gallery_indices']}\")\n",
    "        return False, {\"index_mismatches\": mismatches}\n",
    "    \n",
    "    # Check 3: Verify no duplicates\n",
    "    q_dupes = [id_ for id_ in unique_q if len(q_indices[id_]) > 1]\n",
    "    g_dupes = [id_ for id_ in unique_g if len(g_indices[id_]) > 1]\n",
    "    \n",
    "    if q_dupes or g_dupes:\n",
    "        print(\"Error: Duplicate identity indices found\")\n",
    "        if q_dupes: print(f\"Query duplicates ({len(q_dupes)}):\", q_dupes[:5])\n",
    "        if g_dupes: print(f\"Gallery duplicates ({len(g_dupes)}):\", g_dupes[:5])\n",
    "        return False, {\"query_duplicates\": q_dupes, \"gallery_duplicates\": g_dupes}\n",
    "    \n",
    "    print(\"Success: Identity mappings are consistent\")\n",
    "    print(f\"Query identities: {len(unique_q)}\")\n",
    "    print(f\"Gallery identities: {len(unique_g)}\")\n",
    "    print(f\"Shared identities: {len(set(unique_q) & set(unique_g))}\")\n",
    "    \n",
    "    return True, {\n",
    "        \"query_identities\": len(unique_q),\n",
    "        \"gallery_identities\": len(unique_g),\n",
    "        \"shared_identities\": len(set(unique_q) & set(unique_g))\n",
    "    }\n",
    "\n",
    "def create_query_gallery_splits(\n",
    "    df, \n",
    "    max_queries, \n",
    "    min_gallery, \n",
    "    min_samples, \n",
    "    query_ratio=QUERY_RATIO, \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Create query/gallery splits with guaranteed gallery support.\n",
    "    Tries to ensure queries make up about `query_ratio` of gallery samples.\n",
    "    \"\"\"\n",
    "    results = {'query': [], 'gallery': []}\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    for identity, count in id_counts.items():\n",
    "        samples = df[df['identity'] == identity]\n",
    "        possible_queries = min(\n",
    "            max_queries, \n",
    "            max(1, int(count * query_ratio))\n",
    "        )\n",
    "        if count >= min_samples and possible_queries > 0 and (count - possible_queries) >= min_gallery:\n",
    "            query_samples = samples.sample(possible_queries, random_state=random_state)\n",
    "            results['query'].append(query_samples)\n",
    "            results['gallery'].append(samples.drop(query_samples.index))\n",
    "        else:\n",
    "            results['gallery'].append(samples)\n",
    "    query_df = pd.concat(results['query']) if results['query'] else pd.DataFrame()\n",
    "    gallery_df = pd.concat(results['gallery'])\n",
    "    gallery_df = gallery_df[~gallery_df.index.isin(query_df.index)]\n",
    "    query_df = query_df[query_df['identity'].isin(gallery_df['identity'])]\n",
    "    return query_df, gallery_df\n",
    "\n",
    "def process_test_set(test_set_dir, metadata_path='/home/wellvw12/leopard'):\n",
    "    \"\"\"\n",
    "    Process an individual test set directory to create validated query/gallery splits.\n",
    "    \n",
    "    Args:\n",
    "        test_set_dir: Path to directory containing test set CSV\n",
    "        metadata_path: Path to dataset root for WildlifeDataset initialization\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_status, report_dict)\n",
    "    \"\"\"\n",
    "    # Path setup\n",
    "    test_csv_path = os.path.join(test_set_dir, 'test.csv')\n",
    "    output_query_path = os.path.join(test_set_dir, 'query.csv')\n",
    "    output_gallery_path = os.path.join(test_set_dir, 'gallery.csv')\n",
    "    \n",
    "    if not os.path.exists(test_csv_path):\n",
    "        print(f\"Error: test.csv not found in {test_set_dir}\")\n",
    "        return False, {\"error\": \"test.csv not found\"}\n",
    "    \n",
    "    try:\n",
    "        # Load test set\n",
    "        test_df = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        # Create query/gallery splits\n",
    "        query_df, gallery_df = create_query_gallery_splits(test_df)\n",
    "        \n",
    "        # Verify identity mappings\n",
    "        print(f\"\\nVerifying identity mappings for {test_set_dir}:\")\n",
    "        is_consistent, verification_report = check_identity_mapping(\n",
    "            query_df, gallery_df, metadata_path\n",
    "        )\n",
    "        \n",
    "        if not is_consistent:\n",
    "            print(\"Aborting due to identity mapping issues\")\n",
    "            return False, verification_report\n",
    "        \n",
    "        # Save the validated splits\n",
    "        query_df.to_csv(output_query_path, index=False)\n",
    "        gallery_df.to_csv(output_gallery_path, index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nSuccessfully processed {test_set_dir}:\")\n",
    "        print(f\"  Query samples: {len(query_df)}\")\n",
    "        print(f\"  Gallery samples: {len(gallery_df)}\")\n",
    "        print(f\"  Query identities: {len(query_df['identity'].unique())}\")\n",
    "        print(f\"  Gallery identities: {len(gallery_df['identity'].unique())}\")\n",
    "        \n",
    "        return True, {\n",
    "            \"query_samples\": len(query_df),\n",
    "            \"gallery_samples\": len(gallery_df),\n",
    "            \"query_identities\": len(query_df['identity'].unique()),\n",
    "            \"gallery_identities\": len(gallery_df['identity'].unique()),\n",
    "            \"verification_report\": verification_report\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {test_set_dir}: {str(e)}\")\n",
    "        return False, {\"error\": str(e)}\n",
    "\n",
    "# Load dataset\n",
    "d = LeopardID2022('/home/wellvw12/leopard')\n",
    "df = d.df[~(d.df['identity'] == 'unknown')]\n",
    "\n",
    "# Parameters\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def save_dataframe(df, path):\n",
    "    \"\"\"Safely save DataFrame, creating empty file if DataFrame is empty\"\"\"\n",
    "    if len(df) == 0:\n",
    "        df.head(0).to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "# --- Step 1: First split into Test and Non-Test ---\n",
    "print(\"Creating test set...\")\n",
    "test_identities = np.random.choice(\n",
    "    df['identity'].unique(), \n",
    "    size=int(len(df['identity'].unique()) * SEPERATE_TEST),\n",
    "    replace=False\n",
    ")\n",
    "test_df = df[df['identity'].isin(test_identities)]\n",
    "non_test_df = df[~df['identity'].isin(test_identities)]\n",
    "\n",
    "# --- Step 2: Create Balanced KD Set ---\n",
    "print(\"\\nCreating KD set...\")\n",
    "def create_balanced_kd_set(df, target_samples=KD_TARGET_SAMPLES, \n",
    "                          min_samples=MIN_KD_SAMPLES_PER_ID, \n",
    "                          max_samples=MAX_KD_SAMPLES_PER_ID,\n",
    "                          id_ratio=KD_ID_RATIO):\n",
    "    # Select identities for KD set (prioritize those with more samples)\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    kd_candidate_ids = id_counts[id_counts >= min_samples].index\n",
    "    n_kd_ids = max(int(len(id_counts) * id_ratio), 30)  # At least 30 identities\n",
    "    \n",
    "    # Sort by sample count and take top identities that give us target_samples\n",
    "    sorted_ids = id_counts.sort_values(ascending=False).index\n",
    "    selected_ids = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    for identity in sorted_ids:\n",
    "        if identity in kd_candidate_ids and len(selected_ids) < n_kd_ids:\n",
    "            group = df[df['identity'] == identity]\n",
    "            samples = min(max_samples, len(group))\n",
    "            if total_samples + samples <= target_samples or len(selected_ids) < 30:\n",
    "                selected_ids.append(identity)\n",
    "                total_samples += samples\n",
    "    \n",
    "    # Now sample from selected identities\n",
    "    kd_samples = []\n",
    "    remaining_samples = []\n",
    "    \n",
    "    for identity in df['identity'].unique():\n",
    "        group = df[df['identity'] == identity]\n",
    "        if identity in selected_ids:\n",
    "            n_samples = min(max_samples, max(min_samples, len(group)))\n",
    "            kd_samples.append(group.sample(n=n_samples, random_state=42))\n",
    "            remaining_samples.append(group.drop(kd_samples[-1].index))\n",
    "        else:\n",
    "            remaining_samples.append(group)\n",
    "    \n",
    "    kd_df = pd.concat(kd_samples)\n",
    "    remaining_df = pd.concat(remaining_samples)\n",
    "    \n",
    "    print(f\"KD set: {len(kd_df)} samples, {kd_df['identity'].nunique()} identities\")\n",
    "    print(f\"Avg samples/identity: {len(kd_df)/kd_df['identity'].nunique():.1f}\")\n",
    "    print(f\"Remaining: {len(remaining_df)} samples, {remaining_df['identity'].nunique()} identities\")\n",
    "    return kd_df, remaining_df\n",
    "\n",
    "kd_df, client_df = create_balanced_kd_set(non_test_df)\n",
    "\n",
    "# Save KD set as \"client 0\"\n",
    "kd_dir = f\"{OUTPUT_DIR}/0\"\n",
    "os.makedirs(kd_dir, exist_ok=True)\n",
    "save_dataframe(kd_df, f\"{kd_dir}/train.csv\")\n",
    "save_dataframe(pd.DataFrame(), f\"{kd_dir}/query.csv\")\n",
    "save_dataframe(pd.DataFrame(), f\"{kd_dir}/gallery.csv\")\n",
    "\n",
    "# --- Step 3: Distribute Remaining to Clients ---\n",
    "print(\"\\nDistributing to clients...\")\n",
    "all_identities = client_df['identity'].unique()\n",
    "np.random.seed(42)\n",
    "client_assignments = {i: [] for i in range(1, N_CLIENTS+1)}  # Clients 1-6\n",
    "\n",
    "# ...existing code...\n",
    " # Example: 5 clients, first two are \"smaller\"\n",
    "probs = np.random.dirichlet(DIRICHLET_ALPHAS, size=len(all_identities))\n",
    "\n",
    "client_assignments = {i: [] for i in range(1, N_CLIENTS+1)}\n",
    "for idx, identity in enumerate(all_identities):\n",
    "    client_id = np.random.choice(N_CLIENTS, p=probs[idx]) + 1\n",
    "    client_assignments[client_id].append(identity)\n",
    "# ...existing code...\n",
    "\n",
    "# Rebalance clients\n",
    "for client_id in client_assignments:\n",
    "    client_data = client_df[client_df['identity'].isin(client_assignments[client_id])]\n",
    "    while len(client_data) < MIN_SAMPLES_PER_CLIENT:\n",
    "        richest_client = max(client_assignments.items(), \n",
    "                           key=lambda x: len(client_df[client_df['identity'].isin(x[1])]))[0]\n",
    "        transfer_ids = client_assignments[richest_client][-1:]  # Transfer one identity\n",
    "        client_assignments[richest_client] = client_assignments[richest_client][:-1]\n",
    "        client_assignments[client_id].extend(transfer_ids)\n",
    "        client_data = client_df[client_df['identity'].isin(client_assignments[client_id])]\n",
    "\n",
    "# --- Step 4: Save Test Set as Client 7 ---\n",
    "test_dir = f\"{OUTPUT_DIR}/7\"\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "save_dataframe(test_df, f\"{test_dir}/train.csv\")\n",
    "\n",
    "# Create test query/gallery splits\n",
    "test_query, test_gallery = create_query_gallery_splits(\n",
    "    test_df,\n",
    "    max_queries=MAX_QUERIES_PER_ID,\n",
    "    min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "    min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "    query_ratio=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "save_dataframe(test_query, f\"{test_dir}/query.csv\")\n",
    "save_dataframe(test_gallery, f\"{test_dir}/gallery.csv\")\n",
    "\n",
    "# --- Step 5: Process Each Client ---\n",
    "print(\"\\nProcessing clients...\")\n",
    "used_identities = set()\n",
    "for client_id, identities in client_assignments.items():\n",
    "    client_dir = f\"{OUTPUT_DIR}/{client_id}\"\n",
    "    os.makedirs(client_dir, exist_ok=True)\n",
    "    identities = [id_ for id_ in identities if id_ not in used_identities]\n",
    "    used_identities.update(identities)\n",
    "    client_data = client_df[client_df['identity'].isin(identities)]\n",
    "    \n",
    "    # Split into train/test (15% test)\n",
    "    id_counts = client_data['identity'].value_counts()\n",
    "    multi_sample_ids = id_counts[id_counts > 1].index\n",
    "    \n",
    "    if len(multi_sample_ids) == 0:\n",
    "        train = client_data\n",
    "        test = pd.DataFrame(columns=client_data.columns)\n",
    "    else:\n",
    "        df_multi = client_data[client_data['identity'].isin(multi_sample_ids)]\n",
    "        df_single = client_data[~client_data['identity'].isin(multi_sample_ids)]\n",
    "        \n",
    "        if len(df_multi) < 2:\n",
    "            train = client_data\n",
    "            test = pd.DataFrame(columns=client_data.columns)\n",
    "        else:\n",
    "            try:\n",
    "                train_multi, test_multi = train_test_split(\n",
    "                    df_multi, test_size=TEST_SIZE, \n",
    "                    stratify=df_multi['identity'], random_state=42\n",
    "                )\n",
    "            except ValueError:\n",
    "                train_multi, test_multi = train_test_split(\n",
    "                    df_multi, test_size=TEST_SIZE, random_state=42\n",
    "                )\n",
    "            \n",
    "            train = pd.concat([train_multi, df_single])\n",
    "            test = test_multi\n",
    "    \n",
    "    # Create query/gallery from test set\n",
    "    if len(test) > 0:\n",
    "        query, gallery = create_query_gallery_splits(\n",
    "            test,\n",
    "            max_queries=MAX_QUERIES_PER_ID,\n",
    "            min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "            min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "            query_ratio=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        query = query[query['identity'].isin(gallery['identity'])]\n",
    "        test_id_counts = test['identity'].value_counts()\n",
    "        problematic = test_id_counts[test_id_counts == 1].index\n",
    "        if len(problematic) > 0:\n",
    "            train = pd.concat([train, test[test['identity'].isin(problematic)]])\n",
    "            query = query[~query['identity'].isin(problematic)]\n",
    "            gallery = gallery[~gallery['identity'].isin(problematic)]\n",
    "        print(f\"\\nVerifying identity mapping for Client {client_id}:\")\n",
    "        is_consistent, report = check_identity_mapping(\n",
    "            query, gallery, '/home/wellvw12/leopard'\n",
    "        )\n",
    "    else:\n",
    "        query = pd.DataFrame(columns=client_data.columns)\n",
    "        gallery = pd.DataFrame(columns=client_data.columns)\n",
    "    save_dataframe(train, f\"{client_dir}/train.csv\")\n",
    "    save_dataframe(query, f\"{client_dir}/query.csv\")\n",
    "    save_dataframe(gallery, f\"{client_dir}/gallery.csv\")\n",
    "    print(f\"Client {client_id} saved - Train: {len(train)}, Query: {len(query)}, Gallery: {len(gallery)}\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nFinal Distribution Summary:\")\n",
    "all_identities = defaultdict(list)\n",
    "\n",
    "for client in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if not client.isdigit():\n",
    "        continue\n",
    "        \n",
    "    client_path = f\"{OUTPUT_DIR}/{client}\"\n",
    "    try:\n",
    "        train = pd.read_csv(f\"{client_path}/train.csv\")\n",
    "        query = pd.read_csv(f\"{client_path}/query.csv\") if os.path.getsize(f\"{client_path}/query.csv\") > 0 else pd.DataFrame()\n",
    "        gallery = pd.read_csv(f\"{client_path}/gallery.csv\") if os.path.getsize(f\"{client_path}/gallery.csv\") > 0 else pd.DataFrame()\n",
    "        \n",
    "        identities = train['identity'].unique() if 'identity' in train.columns else []\n",
    "        \n",
    "        for id_ in identities:\n",
    "            all_identities[id_].append(client)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing client {client}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Check for overlapping identities\n",
    "overlaps = {k:v for k,v in all_identities.items() if len(v) > 1}\n",
    "if overlaps:\n",
    "    print(f\"Warning: {len(overlaps)} identities shared across clients\")\n",
    "    print(\"Sample overlaps:\", dict(list(overlaps.items())[:3]))\n",
    "else:\n",
    "    print(\"Success: All identities are unique to their clients\")\n",
    "\n",
    "print(\"\\nClient Statistics:\")\n",
    "stats = []\n",
    "for client in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if not client.isdigit():\n",
    "        continue\n",
    "        \n",
    "    client_path = f\"{OUTPUT_DIR}/{client}\"\n",
    "    try:\n",
    "        train = pd.read_csv(f\"{client_path}/train.csv\")\n",
    "        query = pd.read_csv(f\"{client_path}/query.csv\") if os.path.getsize(f\"{client_path}/query.csv\") > 0 else pd.DataFrame()\n",
    "        gallery = pd.read_csv(f\"{client_path}/gallery.csv\") if os.path.getsize(f\"{client_path}/gallery.csv\") > 0 else pd.DataFrame()\n",
    "        \n",
    "        stats.append({\n",
    "            'Client': client,\n",
    "            'Type': 'KD' if client == '0' else 'Test' if client == '7' else 'Train',\n",
    "            'Identities': train['identity'].nunique() if 'identity' in train.columns else 0,\n",
    "            'Train Samples': len(train),\n",
    "            'Query Samples': len(query),\n",
    "            'Gallery Samples': len(gallery),\n",
    "            'Avg Samples/ID': round(len(train)/train['identity'].nunique(), 1) if 'identity' in train.columns and train['identity'].nunique() > 0 else 0\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error gathering stats for client {client}: {str(e)}\")\n",
    "\n",
    "print(pd.DataFrame(stats).to_markdown(index=False))\n",
    "print(\"\\nKD Set Details:\")\n",
    "print(kd_df['identity'].value_counts().describe())\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
