{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import floor\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021, LeopardID2022, Cows2021v2\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, loader, metrics\n",
    "from wildlife_datasets import splits\n",
    "import os\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/picekl/czechlynx?dataset_version_number=6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.4G/11.4G [01:19<00:00, 154MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/wellvw12/.cache/kagglehub/datasets/picekl/czechlynx/versions/6\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"picekl/czechlynx\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET Cows2021v2: DOWNLOADING STARTED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4vnrca7qw1642qlwxjadp87h7.zip: 0.00B [00:00, ?B/s]\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1455\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1453\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m-> 1455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1375\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCows2021v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCows\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wildlife_datasets/datasets/datasets.py:303\u001b[0m, in \u001b[0;36mWildlifeDataset.get_data\u001b[0;34m(cls, root, force, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATASET \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: DOWNLOADING STARTED.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dataset_name)\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATASET \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: EXTRACTING STARTED.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dataset_name)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mextract(root,  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wildlife_datasets/datasets/datasets.py:335\u001b[0m, in \u001b[0;36mWildlifeDataset.download\u001b[0;34m(cls, root, force, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(mark_file_name)\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mdata_directory(root):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mopen\u001b[39m(mark_file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicenses_url\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msummary:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wildlife_datasets/datasets/cows.py:16\u001b[0m, in \u001b[0;36mCows2021._download\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchive\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/wildlife_datasets/datasets/utils.py:177\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, output_path)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_url\u001b[39m(url, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar(unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, miniters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39murl\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m--> 177\u001b[0m         \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporthook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)>"
     ]
    }
   ],
   "source": [
    "Cows2021v2.get_data('Cows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/home/wellvw12/baselines/baseline3.3.3/0/query.csv')\n",
    "df2 = pd.read_csv('/home/wellvw12/baselines/baseline3.3.3/1/gallery.csv')\n",
    "\n",
    "# missing_ids = df1[~df1['identity'].isin(df2['identity'])]['identity'].unique()\n",
    "\n",
    "# if len(missing_ids) == 0:\n",
    "#     print(\"All query identities exist in the gallery.\")\n",
    "# else:\n",
    "#     print(f\"Missing IDs in gallery: {missing_ids}\")\n",
    "\n",
    "df1.equals(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test set...\n",
      "\n",
      "Creating KD set...\n",
      "KD set: 350 samples, 35 identities\n",
      "Avg samples/identity: 10.0\n",
      "Remaining: 5770 samples, 394 identities\n",
      "\n",
      "Distributing to clients...\n",
      "\n",
      "Processing clients...\n",
      "\n",
      "Verifying identity mapping for Client 1:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 4\n",
      "Gallery identities: 12\n",
      "Shared identities: 4\n",
      "Client 1 saved - Train: 512, Query: 7, Gallery: 68\n",
      "\n",
      "Verifying identity mapping for Client 2:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 17\n",
      "Gallery identities: 23\n",
      "Shared identities: 17\n",
      "Client 2 saved - Train: 1601, Query: 29, Gallery: 212\n",
      "\n",
      "Verifying identity mapping for Client 3:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 18\n",
      "Gallery identities: 31\n",
      "Shared identities: 18\n",
      "Client 3 saved - Train: 1096, Query: 19, Gallery: 140\n",
      "\n",
      "Verifying identity mapping for Client 4:\n",
      "Success: Identity mappings are consistent\n",
      "Query identities: 26\n",
      "Gallery identities: 34\n",
      "Shared identities: 26\n",
      "Client 4 saved - Train: 1832, Query: 33, Gallery: 221\n",
      "\n",
      "Final Distribution Summary:\n",
      "Error processing client 0: No columns to parse from file\n",
      "Success: All identities are unique to their clients\n",
      "\n",
      "Client Statistics:\n",
      "Error gathering stats for client 0: No columns to parse from file\n",
      "|   Client | Type   |   Identities |   Train Samples |   Query Samples |   Gallery Samples |   Avg Samples/ID |\n",
      "|---------:|:-------|-------------:|----------------:|----------------:|------------------:|-----------------:|\n",
      "|        1 | Train  |           48 |             512 |               7 |                68 |             10.7 |\n",
      "|        2 | Train  |           85 |            1601 |              29 |               212 |             18.8 |\n",
      "|        3 | Train  |           96 |            1096 |              19 |               140 |             11.4 |\n",
      "|        4 | Train  |          165 |            1832 |              33 |               221 |             11.1 |\n",
      "|        7 | Test   |           36 |             686 |              47 |               639 |             19.1 |\n",
      "\n",
      "KD Set Details:\n",
      "count    35.0\n",
      "mean     10.0\n",
      "std       0.0\n",
      "min      10.0\n",
      "25%      10.0\n",
      "50%      10.0\n",
      "75%      10.0\n",
      "max      10.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from math import floor\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021v2, LeopardID2022, HyenaID2022\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, loader, metrics\n",
    "from wildlife_datasets import splits\n",
    "import os\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_CLIENTS = 4  # 6 clients + 1 test set\n",
    "OUTPUT_DIR = \"/home/wellvw12/lep_4/clients\"\n",
    "DIRICHLET_ALPHA = 0.5\n",
    "MIN_SAMPLES_PER_CLIENT = 90\n",
    "TEST_SIZE = 0.15\n",
    "QUERY_RATIO = 0.2  # Ratio of query samples to gallery samples\n",
    "MIN_GALLERY_PER_QUERY = 2\n",
    "MAX_QUERIES_PER_ID = 8\n",
    "KD_TARGET_SAMPLES = 350  # Total target samples for KD set\n",
    "MIN_KD_SAMPLES_PER_ID = 3\n",
    "MAX_KD_SAMPLES_PER_ID = 10\n",
    "KD_ID_RATIO = 0.3  # Percentage of identities to include in KD set\n",
    "SEPERATE_TEST = 0.085\n",
    "DIRICHLET_ALPHAS = [0.25, 0.4, 0.5, 0.9] \n",
    "\n",
    "def check_identity_mapping(query_set, gallery_set, metadata_path):\n",
    "    \"\"\"Verify consistent identity indexing between query and gallery sets.\"\"\"\n",
    "    metadata = LeopardID2022(metadata_path)\n",
    "    # metadata = HyenaID2022(metadata_path)\n",
    "    q = WildlifeDataset(query_set, metadata.root)\n",
    "    g = WildlifeDataset(gallery_set, metadata.root)\n",
    "    \n",
    "    q_identities = q.labels_map\n",
    "    g_identities = g.labels_map\n",
    "    \n",
    "    # Check 1: Verify all query identities exist in gallery\n",
    "    unique_q = np.unique(q_identities)\n",
    "    unique_g = np.unique(g_identities)\n",
    "    missing = set(unique_q) - set(unique_g)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"Error: {len(missing)} query identities missing from gallery\")\n",
    "        print(\"First 5 missing:\", list(missing)[:5])\n",
    "        return False, {\"missing_identities\": list(missing)}\n",
    "    \n",
    "    # Check 2: Verify index positions match\n",
    "    mismatches = {}\n",
    "    q_indices = {id_: np.where(q_identities == id_)[0] for id_ in unique_q}\n",
    "    g_indices = {id_: np.where(g_identities == id_)[0] for id_ in unique_g}\n",
    "    \n",
    "    for id_ in set(unique_q) & set(unique_g):\n",
    "        if not np.array_equal(q_indices[id_], g_indices[id_]):\n",
    "            mismatches[id_] = {\n",
    "                \"query_indices\": q_indices[id_].tolist(),\n",
    "                \"gallery_indices\": g_indices[id_].tolist()\n",
    "            }\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"Error: {len(mismatches)} identities have index mismatches\")\n",
    "        for id_, idx in list(mismatches.items())[:3]:\n",
    "            print(f\"{id_}:\\n  Query positions: {idx['query_indices']}\\n  Gallery positions: {idx['gallery_indices']}\")\n",
    "        return False, {\"index_mismatches\": mismatches}\n",
    "    \n",
    "    # Check 3: Verify no duplicates\n",
    "    q_dupes = [id_ for id_ in unique_q if len(q_indices[id_]) > 1]\n",
    "    g_dupes = [id_ for id_ in unique_g if len(g_indices[id_]) > 1]\n",
    "    \n",
    "    if q_dupes or g_dupes:\n",
    "        print(\"Error: Duplicate identity indices found\")\n",
    "        if q_dupes: print(f\"Query duplicates ({len(q_dupes)}):\", q_dupes[:5])\n",
    "        if g_dupes: print(f\"Gallery duplicates ({len(g_dupes)}):\", g_dupes[:5])\n",
    "        return False, {\"query_duplicates\": q_dupes, \"gallery_duplicates\": g_dupes}\n",
    "    \n",
    "    print(\"Success: Identity mappings are consistent\")\n",
    "    print(f\"Query identities: {len(unique_q)}\")\n",
    "    print(f\"Gallery identities: {len(unique_g)}\")\n",
    "    print(f\"Shared identities: {len(set(unique_q) & set(unique_g))}\")\n",
    "    \n",
    "    return True, {\n",
    "        \"query_identities\": len(unique_q),\n",
    "        \"gallery_identities\": len(unique_g),\n",
    "        \"shared_identities\": len(set(unique_q) & set(unique_g))\n",
    "    }\n",
    "\n",
    "def create_query_gallery_splits(\n",
    "    df, \n",
    "    max_queries, \n",
    "    min_gallery, \n",
    "    min_samples, \n",
    "    query_ratio=QUERY_RATIO, \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Create query/gallery splits with guaranteed gallery support.\n",
    "    Tries to ensure queries make up about `query_ratio` of gallery samples.\n",
    "    \"\"\"\n",
    "    results = {'query': [], 'gallery': []}\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    for identity, count in id_counts.items():\n",
    "        samples = df[df['identity'] == identity]\n",
    "        possible_queries = min(\n",
    "            max_queries, \n",
    "            max(1, int(count * query_ratio))\n",
    "        )\n",
    "        if count >= min_samples and possible_queries > 0 and (count - possible_queries) >= min_gallery:\n",
    "            query_samples = samples.sample(possible_queries, random_state=random_state)\n",
    "            results['query'].append(query_samples)\n",
    "            results['gallery'].append(samples.drop(query_samples.index))\n",
    "        else:\n",
    "            results['gallery'].append(samples)\n",
    "    query_df = pd.concat(results['query']) if results['query'] else pd.DataFrame()\n",
    "    gallery_df = pd.concat(results['gallery'])\n",
    "    gallery_df = gallery_df[~gallery_df.index.isin(query_df.index)]\n",
    "    query_df = query_df[query_df['identity'].isin(gallery_df['identity'])]\n",
    "    return query_df, gallery_df\n",
    "\n",
    "def process_test_set(test_set_dir, metadata_path='/home/wellvw12/leopard'):\n",
    "    \"\"\"\n",
    "    Process an individual test set directory to create validated query/gallery splits.\n",
    "    \n",
    "    Args:\n",
    "        test_set_dir: Path to directory containing test set CSV\n",
    "        metadata_path: Path to dataset root for WildlifeDataset initialization\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_status, report_dict)\n",
    "    \"\"\"\n",
    "    # Path setup\n",
    "    test_csv_path = os.path.join(test_set_dir, 'test.csv')\n",
    "    output_query_path = os.path.join(test_set_dir, 'query.csv')\n",
    "    output_gallery_path = os.path.join(test_set_dir, 'gallery.csv')\n",
    "    \n",
    "    if not os.path.exists(test_csv_path):\n",
    "        print(f\"Error: test.csv not found in {test_set_dir}\")\n",
    "        return False, {\"error\": \"test.csv not found\"}\n",
    "    \n",
    "    try:\n",
    "        # Load test set\n",
    "        test_df = pd.read_csv(test_csv_path)\n",
    "        \n",
    "        # Create query/gallery splits\n",
    "        query_df, gallery_df = create_query_gallery_splits(test_df)\n",
    "        \n",
    "        # Verify identity mappings\n",
    "        print(f\"\\nVerifying identity mappings for {test_set_dir}:\")\n",
    "        is_consistent, verification_report = check_identity_mapping(\n",
    "            query_df, gallery_df, metadata_path\n",
    "        )\n",
    "        \n",
    "        if not is_consistent:\n",
    "            print(\"Aborting due to identity mapping issues\")\n",
    "            return False, verification_report\n",
    "        \n",
    "        # Save the validated splits\n",
    "        query_df.to_csv(output_query_path, index=False)\n",
    "        gallery_df.to_csv(output_gallery_path, index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nSuccessfully processed {test_set_dir}:\")\n",
    "        print(f\"  Query samples: {len(query_df)}\")\n",
    "        print(f\"  Gallery samples: {len(gallery_df)}\")\n",
    "        print(f\"  Query identities: {len(query_df['identity'].unique())}\")\n",
    "        print(f\"  Gallery identities: {len(gallery_df['identity'].unique())}\")\n",
    "        \n",
    "        return True, {\n",
    "            \"query_samples\": len(query_df),\n",
    "            \"gallery_samples\": len(gallery_df),\n",
    "            \"query_identities\": len(query_df['identity'].unique()),\n",
    "            \"gallery_identities\": len(gallery_df['identity'].unique()),\n",
    "            \"verification_report\": verification_report\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {test_set_dir}: {str(e)}\")\n",
    "        return False, {\"error\": str(e)}\n",
    "\n",
    "# Load dataset\n",
    "d = LeopardID2022('/home/wellvw12/leopard')\n",
    "# d = HyenaID2022('/home/wellvw12/hyenaid2022')\n",
    "df = d.df[~(d.df['identity'] == 'unknown')]\n",
    "\n",
    "# Parameters\n",
    "\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def save_dataframe(df, path):\n",
    "    \"\"\"Safely save DataFrame, creating empty file if DataFrame is empty\"\"\"\n",
    "    if len(df) == 0:\n",
    "        df.head(0).to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "# --- Step 1: First split into Test and Non-Test ---\n",
    "print(\"Creating test set...\")\n",
    "test_identities = np.random.choice(\n",
    "    df['identity'].unique(), \n",
    "    size=int(len(df['identity'].unique()) * SEPERATE_TEST),\n",
    "    replace=False\n",
    ")\n",
    "test_df = df[df['identity'].isin(test_identities)]\n",
    "non_test_df = df[~df['identity'].isin(test_identities)]\n",
    "\n",
    "# --- Step 2: Create Balanced KD Set ---\n",
    "print(\"\\nCreating KD set...\")\n",
    "def create_balanced_kd_set(df, target_samples=KD_TARGET_SAMPLES, \n",
    "                          min_samples=MIN_KD_SAMPLES_PER_ID, \n",
    "                          max_samples=MAX_KD_SAMPLES_PER_ID,\n",
    "                          id_ratio=KD_ID_RATIO):\n",
    "    # Select identities for KD set (prioritize those with more samples)\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    kd_candidate_ids = id_counts[id_counts >= min_samples].index\n",
    "    n_kd_ids = max(int(len(id_counts) * id_ratio), 30)  # At least 30 identities\n",
    "    \n",
    "    # Sort by sample count and take top identities that give us target_samples\n",
    "    sorted_ids = id_counts.sort_values(ascending=False).index\n",
    "    selected_ids = []\n",
    "    total_samples = 0\n",
    "    \n",
    "    for identity in sorted_ids:\n",
    "        if identity in kd_candidate_ids and len(selected_ids) < n_kd_ids:\n",
    "            group = df[df['identity'] == identity]\n",
    "            samples = min(max_samples, len(group))\n",
    "            if total_samples + samples <= target_samples or len(selected_ids) < 30:\n",
    "                selected_ids.append(identity)\n",
    "                total_samples += samples\n",
    "    \n",
    "    # Now sample from selected identities\n",
    "    kd_samples = []\n",
    "    remaining_samples = []\n",
    "    \n",
    "    for identity in df['identity'].unique():\n",
    "        group = df[df['identity'] == identity]\n",
    "        if identity in selected_ids:\n",
    "            n_samples = min(max_samples, max(min_samples, len(group)))\n",
    "            kd_samples.append(group.sample(n=n_samples, random_state=42))\n",
    "            remaining_samples.append(group.drop(kd_samples[-1].index))\n",
    "        else:\n",
    "            remaining_samples.append(group)\n",
    "    \n",
    "    kd_df = pd.concat(kd_samples)\n",
    "    remaining_df = pd.concat(remaining_samples)\n",
    "    \n",
    "    print(f\"KD set: {len(kd_df)} samples, {kd_df['identity'].nunique()} identities\")\n",
    "    print(f\"Avg samples/identity: {len(kd_df)/kd_df['identity'].nunique():.1f}\")\n",
    "    print(f\"Remaining: {len(remaining_df)} samples, {remaining_df['identity'].nunique()} identities\")\n",
    "    return kd_df, remaining_df\n",
    "\n",
    "kd_df, client_df = create_balanced_kd_set(non_test_df)\n",
    "\n",
    "# Save KD set as \"client 0\"\n",
    "kd_dir = f\"{OUTPUT_DIR}/0\"\n",
    "os.makedirs(kd_dir, exist_ok=True)\n",
    "save_dataframe(kd_df, f\"{kd_dir}/train.csv\")\n",
    "save_dataframe(pd.DataFrame(), f\"{kd_dir}/query.csv\")\n",
    "save_dataframe(pd.DataFrame(), f\"{kd_dir}/gallery.csv\")\n",
    "\n",
    "# --- Step 3: Distribute Remaining to Clients ---\n",
    "print(\"\\nDistributing to clients...\")\n",
    "all_identities = client_df['identity'].unique()\n",
    "np.random.seed(42)\n",
    "client_assignments = {i: [] for i in range(1, N_CLIENTS+1)}  # Clients 1-6\n",
    "\n",
    "# ...existing code...\n",
    " # Example: 5 clients, first two are \"smaller\"\n",
    "probs = np.random.dirichlet(DIRICHLET_ALPHAS, size=len(all_identities))\n",
    "\n",
    "client_assignments = {i: [] for i in range(1, N_CLIENTS+1)}\n",
    "for idx, identity in enumerate(all_identities):\n",
    "    client_id = np.random.choice(N_CLIENTS, p=probs[idx]) + 1\n",
    "    client_assignments[client_id].append(identity)\n",
    "# ...existing code...\n",
    "\n",
    "# Rebalance clients\n",
    "for client_id in client_assignments:\n",
    "    client_data = client_df[client_df['identity'].isin(client_assignments[client_id])]\n",
    "    while len(client_data) < MIN_SAMPLES_PER_CLIENT:\n",
    "        richest_client = max(client_assignments.items(), \n",
    "                           key=lambda x: len(client_df[client_df['identity'].isin(x[1])]))[0]\n",
    "        transfer_ids = client_assignments[richest_client][-1:]  # Transfer one identity\n",
    "        client_assignments[richest_client] = client_assignments[richest_client][:-1]\n",
    "        client_assignments[client_id].extend(transfer_ids)\n",
    "        client_data = client_df[client_df['identity'].isin(client_assignments[client_id])]\n",
    "\n",
    "# --- Step 4: Save Test Set as Client 7 ---\n",
    "test_dir = f\"{OUTPUT_DIR}/7\"\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "save_dataframe(test_df, f\"{test_dir}/train.csv\")\n",
    "\n",
    "# Create test query/gallery splits\n",
    "test_query, test_gallery = create_query_gallery_splits(\n",
    "    test_df,\n",
    "    max_queries=MAX_QUERIES_PER_ID,\n",
    "    min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "    min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "    query_ratio=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "save_dataframe(test_query, f\"{test_dir}/query.csv\")\n",
    "save_dataframe(test_gallery, f\"{test_dir}/gallery.csv\")\n",
    "\n",
    "# --- Step 5: Process Each Client ---\n",
    "print(\"\\nProcessing clients...\")\n",
    "used_identities = set()\n",
    "for client_id, identities in client_assignments.items():\n",
    "    client_dir = f\"{OUTPUT_DIR}/{client_id}\"\n",
    "    os.makedirs(client_dir, exist_ok=True)\n",
    "    identities = [id_ for id_ in identities if id_ not in used_identities]\n",
    "    used_identities.update(identities)\n",
    "    client_data = client_df[client_df['identity'].isin(identities)]\n",
    "    \n",
    "    # Split into train/test (15% test)\n",
    "    id_counts = client_data['identity'].value_counts()\n",
    "    multi_sample_ids = id_counts[id_counts > 1].index\n",
    "    \n",
    "    if len(multi_sample_ids) == 0:\n",
    "        train = client_data\n",
    "        test = pd.DataFrame(columns=client_data.columns)\n",
    "    else:\n",
    "        df_multi = client_data[client_data['identity'].isin(multi_sample_ids)]\n",
    "        df_single = client_data[~client_data['identity'].isin(multi_sample_ids)]\n",
    "        \n",
    "        if len(df_multi) < 2:\n",
    "            train = client_data\n",
    "            test = pd.DataFrame(columns=client_data.columns)\n",
    "        else:\n",
    "            try:\n",
    "                train_multi, test_multi = train_test_split(\n",
    "                    df_multi, test_size=TEST_SIZE, \n",
    "                    stratify=df_multi['identity'], random_state=42\n",
    "                )\n",
    "            except ValueError:\n",
    "                train_multi, test_multi = train_test_split(\n",
    "                    df_multi, test_size=TEST_SIZE, random_state=42\n",
    "                )\n",
    "            \n",
    "            train = pd.concat([train_multi, df_single])\n",
    "            test = test_multi\n",
    "    \n",
    "    # Create query/gallery from test set\n",
    "    if len(test) > 0:\n",
    "        query, gallery = create_query_gallery_splits(\n",
    "            test,\n",
    "            max_queries=MAX_QUERIES_PER_ID,\n",
    "            min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "            min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "            query_ratio=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        query = query[query['identity'].isin(gallery['identity'])]\n",
    "        test_id_counts = test['identity'].value_counts()\n",
    "        problematic = test_id_counts[test_id_counts == 1].index\n",
    "        if len(problematic) > 0:\n",
    "            train = pd.concat([train, test[test['identity'].isin(problematic)]])\n",
    "            query = query[~query['identity'].isin(problematic)]\n",
    "            gallery = gallery[~gallery['identity'].isin(problematic)]\n",
    "        print(f\"\\nVerifying identity mapping for Client {client_id}:\")\n",
    "        is_consistent, report = check_identity_mapping(\n",
    "            query, gallery, '/home/wellvw12/leopard'\n",
    "        )\n",
    "    else:\n",
    "        query = pd.DataFrame(columns=client_data.columns)\n",
    "        gallery = pd.DataFrame(columns=client_data.columns)\n",
    "    save_dataframe(train, f\"{client_dir}/train.csv\")\n",
    "    save_dataframe(query, f\"{client_dir}/query.csv\")\n",
    "    save_dataframe(gallery, f\"{client_dir}/gallery.csv\")\n",
    "    print(f\"Client {client_id} saved - Train: {len(train)}, Query: {len(query)}, Gallery: {len(gallery)}\")\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nFinal Distribution Summary:\")\n",
    "all_identities = defaultdict(list)\n",
    "\n",
    "for client in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if not client.isdigit():\n",
    "        continue\n",
    "        \n",
    "    client_path = f\"{OUTPUT_DIR}/{client}\"\n",
    "    try:\n",
    "        train = pd.read_csv(f\"{client_path}/train.csv\")\n",
    "        query = pd.read_csv(f\"{client_path}/query.csv\") if os.path.getsize(f\"{client_path}/query.csv\") > 0 else pd.DataFrame()\n",
    "        gallery = pd.read_csv(f\"{client_path}/gallery.csv\") if os.path.getsize(f\"{client_path}/gallery.csv\") > 0 else pd.DataFrame()\n",
    "        \n",
    "        identities = train['identity'].unique() if 'identity' in train.columns else []\n",
    "        \n",
    "        for id_ in identities:\n",
    "            all_identities[id_].append(client)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing client {client}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Check for overlapping identities\n",
    "overlaps = {k:v for k,v in all_identities.items() if len(v) > 1}\n",
    "if overlaps:\n",
    "    print(f\"Warning: {len(overlaps)} identities shared across clients\")\n",
    "    print(\"Sample overlaps:\", dict(list(overlaps.items())[:3]))\n",
    "else:\n",
    "    print(\"Success: All identities are unique to their clients\")\n",
    "\n",
    "print(\"\\nClient Statistics:\")\n",
    "stats = []\n",
    "for client in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if not client.isdigit():\n",
    "        continue\n",
    "        \n",
    "    client_path = f\"{OUTPUT_DIR}/{client}\"\n",
    "    try:\n",
    "        train = pd.read_csv(f\"{client_path}/train.csv\")\n",
    "        query = pd.read_csv(f\"{client_path}/query.csv\") if os.path.getsize(f\"{client_path}/query.csv\") > 0 else pd.DataFrame()\n",
    "        gallery = pd.read_csv(f\"{client_path}/gallery.csv\") if os.path.getsize(f\"{client_path}/gallery.csv\") > 0 else pd.DataFrame()\n",
    "        \n",
    "        stats.append({\n",
    "            'Client': client,\n",
    "            'Type': 'KD' if client == '0' else 'Test' if client == '7' else 'Train',\n",
    "            'Identities': train['identity'].nunique() if 'identity' in train.columns else 0,\n",
    "            'Train Samples': len(train),\n",
    "            'Query Samples': len(query),\n",
    "            'Gallery Samples': len(gallery),\n",
    "            'Avg Samples/ID': round(len(train)/train['identity'].nunique(), 1) if 'identity' in train.columns and train['identity'].nunique() > 0 else 0\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error gathering stats for client {client}: {str(e)}\")\n",
    "\n",
    "print(pd.DataFrame(stats).to_markdown(index=False))\n",
    "print(\"\\nKD Set Details:\")\n",
    "print(kd_df['identity'].value_counts().describe())\n",
    "print(\"\\nData preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from wildlife_datasets.datasets import HyenaID2022\n",
    "\n",
    "# Global parameters\n",
    "N_CLIENTS = 4\n",
    "OUTPUT_DIR = \"/home/wellvw12/hyena_6/clients\"\n",
    "DIRICHLET_ALPHA = 0.5\n",
    "MIN_SAMPLES_PER_CLIENT = 90\n",
    "TEST_SIZE = 0.25\n",
    "QUERY_RATIO = 0.2\n",
    "MIN_GALLERY_PER_QUERY = 2\n",
    "MAX_QUERIES_PER_ID = 8\n",
    "SEPERATE_TEST = 0.085\n",
    "DIRICHLET_ALPHAS = [0.45, 0.45, 0.6, 0.9]\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "def create_query_gallery_splits(\n",
    "    df, \n",
    "    max_queries=MAX_QUERIES_PER_ID, \n",
    "    min_gallery=MIN_GALLERY_PER_QUERY, \n",
    "    min_samples=MIN_GALLERY_PER_QUERY+1, \n",
    "    query_ratio=QUERY_RATIO, \n",
    "    random_state=RANDOM_SEED\n",
    "):\n",
    "    \"\"\"\n",
    "    Create query/gallery splits ensuring query images are in gallery.\n",
    "    \"\"\"\n",
    "    results = {'query': [], 'gallery': []}\n",
    "    id_counts = df['identity'].value_counts()\n",
    "    \n",
    "    for identity, count in id_counts.items():\n",
    "        samples = df[df['identity'] == identity]\n",
    "        \n",
    "        # Calculate number of query samples\n",
    "        possible_queries = min(\n",
    "            max_queries, \n",
    "            max(1, int(count * query_ratio))\n",
    "        )\n",
    "        \n",
    "        # Only create queries if we have enough samples for gallery support\n",
    "        if count >= min_samples and possible_queries > 0 and (count - possible_queries) >= min_gallery:\n",
    "            # Sample query images\n",
    "            query_samples = samples.sample(possible_queries, random_state=random_state)\n",
    "            results['query'].append(query_samples)\n",
    "            \n",
    "            # Gallery includes ALL samples (including query images)\n",
    "            results['gallery'].append(samples)\n",
    "        else:\n",
    "            # If not enough samples for queries, all go to gallery\n",
    "            results['gallery'].append(samples)\n",
    "    \n",
    "    query_df = pd.concat(results['query']) if results['query'] else pd.DataFrame()\n",
    "    gallery_df = pd.concat(results['gallery'])\n",
    "    \n",
    "    # Ensure query images are present in gallery\n",
    "    if len(query_df) > 0:\n",
    "        query_df = query_df[query_df['identity'].isin(gallery_df['identity'])]\n",
    "    \n",
    "    return query_df, gallery_df\n",
    "\n",
    "def create_orientation_based_distribution(df, n_clients=N_CLIENTS):\n",
    "    \"\"\"\n",
    "    Create non-IID distribution based on orientation with realistic heterogeneity.\n",
    "    \"\"\"\n",
    "    print(\"Creating orientation-based non-IID distribution...\")\n",
    "    \n",
    "    # Check if orientation column exists\n",
    "    if 'orientation' not in df.columns:\n",
    "        print(\"Error: No 'orientation' column found in dataset\")\n",
    "        return None\n",
    "    \n",
    "    # Get orientation distribution\n",
    "    orientation_counts = df['orientation'].value_counts()\n",
    "    print(\"Orientation distribution:\")\n",
    "    print(orientation_counts)\n",
    "    \n",
    "    # Define client-specific orientation distributions for strong non-IID\n",
    "    client_distributions = {\n",
    "        1: {  # Right-dominant client\n",
    "            'right': 1000,\n",
    "            'left': 200,\n",
    "            'frontright': 50,\n",
    "            'backright': 50\n",
    "        },\n",
    "        2: {  # Left-dominant client\n",
    "            'left': 1000,\n",
    "            'right': 200,\n",
    "            'frontleft': 80,\n",
    "            'backleft': 40\n",
    "        },\n",
    "        3: {  # Front-focused client\n",
    "            'frontleft': 51,\n",
    "            'frontright': 33,\n",
    "            'front': 51,\n",
    "            'right': 142,\n",
    "            'left': 94\n",
    "        },\n",
    "        4: {  # Rare orientations client\n",
    "            'up': 50,\n",
    "            'back': 20,\n",
    "            'down': 10,\n",
    "            'backleft': 27,\n",
    "            'backright': 31\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    client_assignments = {i: [] for i in range(1, n_clients+1)}\n",
    "    assigned_samples = set()\n",
    "    \n",
    "    # Assign samples based on orientation preferences\n",
    "    for client_id, preferences in client_distributions.items():\n",
    "        if client_id > n_clients:\n",
    "            continue\n",
    "            \n",
    "        client_samples = []\n",
    "        \n",
    "        for orientation, target_count in preferences.items():\n",
    "            if orientation in df['orientation'].values:\n",
    "                # Get available samples for this orientation\n",
    "                orientation_samples = df[\n",
    "                    (df['orientation'] == orientation) & \n",
    "                    (~df.index.isin(assigned_samples))\n",
    "                ]\n",
    "                \n",
    "                # Sample up to target_count or all available\n",
    "                n_samples = min(target_count, len(orientation_samples))\n",
    "                if n_samples > 0:\n",
    "                    sampled = orientation_samples.sample(n=n_samples, random_state=RANDOM_SEED)\n",
    "                    client_samples.append(sampled)\n",
    "                    assigned_samples.update(sampled.index)\n",
    "        \n",
    "        if client_samples:\n",
    "            client_data = pd.concat(client_samples)\n",
    "            client_assignments[client_id] = client_data.index.tolist()\n",
    "            print(f\"Client {client_id}: {len(client_data)} samples\")\n",
    "            print(f\"  Orientations: {client_data['orientation'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Handle remaining unassigned samples\n",
    "    remaining_samples = df[~df.index.isin(assigned_samples)]\n",
    "    if len(remaining_samples) > 0:\n",
    "        print(f\"Distributing {len(remaining_samples)} remaining samples...\")\n",
    "        \n",
    "        # Find client with fewest samples\n",
    "        client_sizes = {i: len(client_assignments[i]) for i in range(1, n_clients+1)}\n",
    "        smallest_client = min(client_sizes, key=client_sizes.get)\n",
    "        \n",
    "        client_assignments[smallest_client].extend(remaining_samples.index.tolist())\n",
    "        print(f\"Added {len(remaining_samples)} samples to Client {smallest_client}\")\n",
    "    \n",
    "    return client_assignments\n",
    "\n",
    "def save_dataframe(df, path):\n",
    "    \"\"\"Safely save DataFrame, creating empty file if DataFrame is empty\"\"\"\n",
    "    if len(df) == 0:\n",
    "        df.head(0).to_csv(path, index=False)\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading HyenaID2022 dataset...\")\n",
    "d = HyenaID2022('/home/wellvw12/hyenaid2022')\n",
    "df = d.df[~(d.df['identity'] == 'unknown')]\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total identities: {df['identity'].nunique()}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Step 1: Create global test set\n",
    "print(\"\\nCreating global test set...\")\n",
    "test_identities = np.random.choice(\n",
    "    df['identity'].unique(), \n",
    "    size=int(len(df['identity'].unique()) * SEPERATE_TEST),\n",
    "    replace=False\n",
    ")\n",
    "test_df = df[df['identity'].isin(test_identities)]\n",
    "client_df = df[~df['identity'].isin(test_identities)]\n",
    "\n",
    "print(f\"Test set: {len(test_df)} samples, {test_df['identity'].nunique()} identities\")\n",
    "print(f\"Client data: {len(client_df)} samples, {client_df['identity'].nunique()} identities\")\n",
    "\n",
    "# Step 2: Create orientation-based client assignments\n",
    "client_assignments = create_orientation_based_distribution(client_df, N_CLIENTS)\n",
    "\n",
    "if client_assignments is None:\n",
    "    print(\"Error: Could not create orientation-based distribution\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 3: Create and save client data\n",
    "print(\"\\nCreating client datasets...\")\n",
    "for client_id in range(1, N_CLIENTS+1):\n",
    "    client_dir = f\"{OUTPUT_DIR}/{client_id}\"\n",
    "    os.makedirs(client_dir, exist_ok=True)\n",
    "    \n",
    "    # Get client data\n",
    "    client_indices = client_assignments[client_id]\n",
    "    client_data = client_df.loc[client_indices]\n",
    "    \n",
    "    # Create query/gallery splits\n",
    "    query_df, gallery_df = create_query_gallery_splits(\n",
    "        client_data,\n",
    "        max_queries=MAX_QUERIES_PER_ID,\n",
    "        min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "        min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "        query_ratio=QUERY_RATIO,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Save client data\n",
    "    save_dataframe(client_data, f\"{client_dir}/train.csv\")\n",
    "    save_dataframe(query_df, f\"{client_dir}/query.csv\")\n",
    "    save_dataframe(gallery_df, f\"{client_dir}/gallery.csv\")\n",
    "    \n",
    "    print(f\"Client {client_id} saved:\")\n",
    "    print(f\"  Train: {len(client_data)} samples, {client_data['identity'].nunique()} identities\")\n",
    "    print(f\"  Query: {len(query_df)} samples\")\n",
    "    print(f\"  Gallery: {len(gallery_df)} samples\")\n",
    "    if len(client_data) > 0:\n",
    "        print(f\"  Top orientations: {client_data['orientation'].value_counts().head(3).to_dict()}\")\n",
    "\n",
    "# Step 4: Create global test set with query/gallery splits\n",
    "print(\"\\nCreating global test set...\")\n",
    "test_dir = f\"{OUTPUT_DIR}/test\"\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "test_query_df, test_gallery_df = create_query_gallery_splits(\n",
    "    test_df,\n",
    "    max_queries=MAX_QUERIES_PER_ID,\n",
    "    min_gallery=MIN_GALLERY_PER_QUERY,\n",
    "    min_samples=MIN_GALLERY_PER_QUERY+1,\n",
    "    query_ratio=0.3,  # Higher ratio for test set\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "save_dataframe(test_df, f\"{test_dir}/all.csv\")\n",
    "save_dataframe(test_query_df, f\"{test_dir}/query.csv\")\n",
    "save_dataframe(test_gallery_df, f\"{test_dir}/gallery.csv\")\n",
    "\n",
    "print(f\"Global test set saved:\")\n",
    "print(f\"  All: {len(test_df)} samples, {test_df['identity'].nunique()} identities\")\n",
    "print(f\"  Query: {len(test_query_df)} samples\")\n",
    "print(f\"  Gallery: {len(test_gallery_df)} samples\")\n",
    "\n",
    "# Step 5: Verification\n",
    "print(\"\\nVerification:\")\n",
    "\n",
    "# Check for identity overlap between clients\n",
    "all_client_identities = set()\n",
    "identity_overlaps = []\n",
    "\n",
    "for client_id in range(1, N_CLIENTS+1):\n",
    "    client_path = f\"{OUTPUT_DIR}/{client_id}/train.csv\"\n",
    "    if os.path.exists(client_path):\n",
    "        client_train = pd.read_csv(client_path)\n",
    "        client_identities = set(client_train['identity'].unique())\n",
    "        \n",
    "        # Check for overlap with previous clients\n",
    "        overlap = all_client_identities.intersection(client_identities)\n",
    "        if overlap:\n",
    "            identity_overlaps.append(f\"Client {client_id} overlaps: {len(overlap)} identities\")\n",
    "        \n",
    "        all_client_identities.update(client_identities)\n",
    "\n",
    "if identity_overlaps:\n",
    "    print(\"Warning: Identity overlaps detected:\")\n",
    "    for overlap in identity_overlaps:\n",
    "        print(f\"  {overlap}\")\n",
    "else:\n",
    "    print(\"✓ No identity overlaps between clients\")\n",
    "\n",
    "# Verify query images are in gallery\n",
    "print(\"\\nVerifying query images are in gallery:\")\n",
    "for client_id in range(1, N_CLIENTS+1):\n",
    "    query_path = f\"{OUTPUT_DIR}/{client_id}/query.csv\"\n",
    "    gallery_path = f\"{OUTPUT_DIR}/{client_id}/gallery.csv\"\n",
    "    \n",
    "    if os.path.exists(query_path) and os.path.exists(gallery_path):\n",
    "        query_df = pd.read_csv(query_path)\n",
    "        gallery_df = pd.read_csv(gallery_path)\n",
    "        \n",
    "        if len(query_df) > 0 and len(gallery_df) > 0:\n",
    "            query_images = set(query_df['image_id'] if 'image_id' in query_df.columns else query_df.index)\n",
    "            gallery_images = set(gallery_df['image_id'] if 'image_id' in gallery_df.columns else gallery_df.index)\n",
    "            \n",
    "            missing = query_images - gallery_images\n",
    "            if missing:\n",
    "                print(f\"  Client {client_id}: {len(missing)} query images missing from gallery\")\n",
    "            else:\n",
    "                print(f\"  Client {client_id}: ✓ All query images in gallery\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Clients created: {N_CLIENTS}\")\n",
    "print(f\"Global test set created: {len(test_df)} samples\")\n",
    "print(f\"Random seed used: {RANDOM_SEED}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
