{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "\n",
    "from fedn.utils.helpers.helpers import save_metrics\n",
    "\n",
    "from torch.optim import SGD\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
    "import itertools\n",
    "\n",
    "from fedn.utils.helpers.helpers import save_metadata\n",
    "\n",
    "\n",
    "from math import floor\n",
    "import torch\n",
    "import torchvision\n",
    "import collections\n",
    "import timm\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "import random\n",
    "from wildlife_datasets.datasets import MacaqueFaces, Cows2021v2, LeopardID2022\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, loader, metrics\n",
    "from wildlife_datasets import splits\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from fedn.utils.helpers.helpers import get_helper\n",
    "\n",
    "HELPER_MODULE = \"numpyhelper\"\n",
    "helper = get_helper(HELPER_MODULE)\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch._utils\n",
    "try:\n",
    "    torch._utils._rebuild_tensor_v2\n",
    "except AttributeError:\n",
    "    def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):\n",
    "        tensor = torch._utils._rebuild_tensor(storage, storage_offset, size, stride)\n",
    "        tensor.requires_grad = requires_grad\n",
    "        tensor._backward_hooks = backward_hooks\n",
    "        return tensor\n",
    "    torch._utils._rebuild_tensor_v2 = _rebuild_tensor_v2\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "# import pretrainedmodels\n",
    "\n",
    "######################################################################\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in') # For old pytorch, you may use kaiming_normal.\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_out')\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('BatchNorm1d') != -1:\n",
    "        init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "def weights_init_classifier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        init.normal_(m.weight.data, std=0.001)\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# Defines the new fc layer and classification layer\n",
    "# |--Linear--|--bn--|--relu--|--Linear--|\n",
    "class ClassBlock(nn.Module):\n",
    "    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True, return_f = False):\n",
    "        super(ClassBlock, self).__init__()\n",
    "        self.return_f = return_f\n",
    "        add_block = []\n",
    "        if linear:\n",
    "            add_block += [nn.Linear(input_dim, num_bottleneck)]\n",
    "        else:\n",
    "            num_bottleneck = input_dim\n",
    "        if bnorm:\n",
    "            add_block += [nn.BatchNorm1d(num_bottleneck)]\n",
    "        if relu:\n",
    "            add_block += [nn.LeakyReLU(0.1)]\n",
    "        if droprate>0:\n",
    "            add_block += [nn.Dropout(p=droprate)]\n",
    "        add_block = nn.Sequential(*add_block)\n",
    "        add_block.apply(weights_init_kaiming)\n",
    "\n",
    "        classifier = []\n",
    "        classifier += [nn.Linear(num_bottleneck, class_num)]\n",
    "        classifier = nn.Sequential(*classifier)\n",
    "        classifier.apply(weights_init_classifier)\n",
    "\n",
    "        self.add_block = add_block\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x):\n",
    "        x = self.add_block(x)\n",
    "        if self.return_f:\n",
    "            f = x\n",
    "            x = self.classifier(x)\n",
    "            return x,f\n",
    "        else:\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "\n",
    "# Define the ResNet50-based Model\n",
    "class ft_net(nn.Module):\n",
    "\n",
    "    def __init__(self, class_num, droprate=0.5, stride=2):\n",
    "        super(ft_net, self).__init__()\n",
    "       \n",
    "        model_ft = models.resnet50(pretrained=True)\n",
    "        # model_ft=torch.load('saved_res50.pkl')\n",
    "        # avg pooling to global pooling\n",
    "        if stride == 1:\n",
    "            model_ft.layer4[0].downsample[0].stride = (1,1)\n",
    "            model_ft.layer4[0].conv2.stride = (1,1)\n",
    "        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.model = model_ft\n",
    "        self.classifier = ClassBlock(2048, class_num, droprate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = self.model.avgpool(x)\n",
    "        x = x.view(x.size(0), x.size(1))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "# sys.path.append(os.path.abspath(dir_path))\n",
    "def full_data(data_path, is_query = True):\n",
    "    # splitter = splits.ClosedSetSplit(0.8)\n",
    "    metadata = LeopardID2022('/home/wellvw12/')\n",
    "    # idx_train, idx_test = splitter.split(metadata.df)[0]\n",
    "    # df_train = metadata.df.loc[idx_train]\n",
    "    # df_test = metadata.df.loc[idx_test]\n",
    "\n",
    "    transform = T.Compose([T.Resize([256, 128]), \n",
    "                       T.ToTensor(), \n",
    "                    #    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                    #    T.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "                    ])\n",
    "    if is_query:\n",
    "        df = pd.read_csv(data_path)\n",
    "        return WildlifeDataset(df,metadata.root, transform=transform)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "        return WildlifeDataset(df,metadata.root, transform=transform)\n",
    "\n",
    "def load_data(data_path, is_train=True):\n",
    "    \"\"\"Load data from disk.\n",
    "    :param data_path: Path to data file.\n",
    "    :type data_path: str\n",
    "    :param is_train: Whether to load training or test data.\n",
    "    :type is_train: bool\n",
    "    :return: Tuple of data and labels.\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    transform = T.Compose([T.Resize([224, 224]), \n",
    "                       T.ToTensor(), \n",
    "                       T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                       T.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "                    ])\n",
    "\n",
    "    # print(data_path)\n",
    "    metadata = LeopardID2022('/home/wellvw12/')\n",
    "\n",
    "    if is_train:\n",
    "        df = pd.read_csv(data_path)\n",
    "        return WildlifeDataset(df,metadata.root, transform=transform)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path)\n",
    "        return WildlifeDataset(df,metadata.root, transform=transform)\n",
    "    \n",
    "# def compile_model():\n",
    "#     \"\"\"Compile the pytorch model.\n",
    "\n",
    "#     :return: The compiled model.\n",
    "#     :rtype: torch.nn.Module\n",
    "#     \"\"\"\n",
    "#     # return timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=False)\n",
    "#     return timm.create_model('resnet18', num_classes=0, pretrained=True)\n",
    "\n",
    "def compile_model(class_num=751, droprate=0.5, stride=2):\n",
    "    \"\"\"Create an empty ft_net model with the same architecture as used during training\n",
    "    \n",
    "    Args:\n",
    "        class_num (int): Number of classes (must match original training)\n",
    "        droprate (float): Dropout rate (must match original training)\n",
    "        stride (int): Stride parameter (must match original training)\n",
    "        \n",
    "    Returns:\n",
    "        ft_net: The uncompiled model with random initialization\n",
    "    \"\"\"\n",
    "    net = ft_net(751, stride=1)\n",
    "\n",
    "    # model = ft_net(class_num=class_num, droprate=droprate, stride=stride)\n",
    "    \n",
    "    # Important: Remove the classifier if your saved parameters don't include it\n",
    "    # (Uncomment if needed)\n",
    "    # net.classifier = nn.Sequential() \n",
    "\n",
    "    # input = Variable(torch.FloatTensor(8, 3, 256, 128))\n",
    "    # output = net(input)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def save_parameters(model, out_path):\n",
    "    \"\"\"Save model paramters to file.\n",
    "\n",
    "    :param model: The model to serialize.\n",
    "    :type model: torch.nn.Module\n",
    "    :param out_path: The path to save to.\n",
    "    :type out_path: str\n",
    "    \"\"\"\n",
    "    parameters_np = [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "    helper.save(parameters_np, out_path)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_parameters(model_path, num_classes=430, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load federated model parameters while handling architecture mismatches.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to saved model file\n",
    "        num_classes (int): Number of output classes\n",
    "        device (str): Device to load model onto ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Module: Loaded model in evaluation mode\n",
    "    \"\"\"\n",
    "    # Initialize model with correct architecture\n",
    "    model = ft_net(num_classes).to(device)\n",
    "    \n",
    "    # Load saved state dict\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Handle different save formats\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "    \n",
    "    # Create filtered state dict matching current model architecture\n",
    "    model_state_dict = model.state_dict()\n",
    "    \n",
    "    # 1. Handle classifier mismatch (remove classifier if needed)\n",
    "    if 'classifier.classifier.0.weight' in state_dict:\n",
    "        if isinstance(model.classifier.classifier, nn.Sequential):\n",
    "            # Remove classifier weights from loaded state dict\n",
    "            state_dict = {k: v for k, v in state_dict.items() \n",
    "                         if not k.startswith('classifier.classifier')}\n",
    "    \n",
    "    # 2. Filter only matching parameters\n",
    "    filtered_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k in model_state_dict:\n",
    "            if v.size() == model_state_dict[k].size():\n",
    "                filtered_state_dict[k] = v\n",
    "            else:\n",
    "                print(f\"Size mismatch for {k}: loaded {v.size()}, model {model_state_dict[k].size()}\")\n",
    "        else:\n",
    "            print(f\"Skipping unexpected key: {k}\")\n",
    "    \n",
    "    # Load filtered state dict\n",
    "    model.load_state_dict(filtered_state_dict, strict=False)\n",
    "    \n",
    "    # Ensure classifier is empty if needed\n",
    "    if isinstance(model.classifier.classifier, nn.Sequential):\n",
    "        model.classifier.classifier = nn.Sequential()\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def init_seed(out_path=\"seed.npz\"):\n",
    "    \"\"\"Initialize seed model and save it to file.\n",
    "\n",
    "    :param out_path: The path to save the seed model to.\n",
    "    :type out_path: str\n",
    "    \"\"\"\n",
    "    # Init and save\n",
    "    model = compile_model()\n",
    "    save_parameters(model, out_path)\n",
    "    \n",
    "import importlib\n",
    "import json\n",
    "\n",
    "HELPER_PLUGIN_PATH = \"fedn.utils.helpers.plugins.{}\"\n",
    "\n",
    "\n",
    "def get_helper(helper_module_name):\n",
    "    \"\"\"Return an instance of the helper class.\n",
    "\n",
    "    :param helper_module_name: The name of the helper plugin module.\n",
    "    :type helper_module_name: str\n",
    "    :return: A helper instance.\n",
    "    :rtype: class: `fedn.utils.helpers.helpers.HelperBase`\n",
    "    \"\"\"\n",
    "    helper_plugin = HELPER_PLUGIN_PATH.format(helper_module_name)\n",
    "    helper = importlib.import_module(helper_plugin)\n",
    "    return helper.Helper()\n",
    "\n",
    "\n",
    "def save_metadata(metadata, filename):\n",
    "    \"\"\"Save metadata to file.\n",
    "\n",
    "    :param metadata: The metadata to save.\n",
    "    :type metadata: dict\n",
    "    :param filename: The name of the file to save to.\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    with open(filename + \"-metadata\", \"w\") as outfile:\n",
    "        json.dump(metadata, outfile)\n",
    "\n",
    "\n",
    "def load_metadata(filename):\n",
    "    \"\"\"Load metadata from file.\n",
    "\n",
    "    :param filename: The name of the file to load from.\n",
    "    :type filename: str\n",
    "    :return: The loaded metadata.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    with open(filename + \"-metadata\", \"r\") as infile:\n",
    "        metadata = json.load(infile)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_metrics(metrics, filename):\n",
    "    \"\"\"Save metrics to file.\n",
    "\n",
    "    :param metrics: The metrics to save.\n",
    "    :type metrics: dict\n",
    "    :param filename: The name of the file to save to.\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        json.dump(metrics, outfile)\n",
    "        \n",
    "def train(in_model_path, out_model_path, data_path=None, batch_size=32, epochs=1):\n",
    "    \"\"\"Complete a model update.\n",
    "\n",
    "    Load model paramters from in_model_path (managed by the FEDn client),\n",
    "    perform a model update, and write updated paramters\n",
    "    to out_model_path (picked up by the FEDn client).\n",
    "\n",
    "    :param in_model_path: The path to the input model.\n",
    "    :type in_model_path: str\n",
    "    :param out_model_path: The path to save the output model to.\n",
    "    :type out_model_path: str\n",
    "    :param data_path: The path to the data file.\n",
    "    :type data_path: str\n",
    "    :param batch_size: The batch size to use.\n",
    "    :type batch_size: int\n",
    "    :param epochs: The number of epochs to train.\n",
    "    :type epochs: int\n",
    "    :param lr: The learning rate to use.\n",
    "    :type lr: float\n",
    "    \"\"\"\n",
    "    # Load data to return wilflifedataset\n",
    "    # data_path: FEDN_DATA_PATH= ./data/clients/1/\n",
    "    if data_path is None:\n",
    "        data_path = os.environ.get(\"FEDN_DATA_PATH\")\n",
    "\n",
    "\n",
    "    # lr = 0.001\n",
    "    torch.cuda.empty_cache()\n",
    "    x_train = load_data(data_path + 'train.csv')\n",
    "    # x_train = full_data(data_path + 'train.csv')\n",
    "    # Load parmeters and initialize model\n",
    "\n",
    "    model = load_parameters(in_model_path)\n",
    "    # Train\n",
    "    lr =0.0002\n",
    "    objective = ArcFaceLoss(\n",
    "        num_classes=x_train.num_classes,\n",
    "        embedding_size=768,\n",
    "        margin=0.5,\n",
    "        scale=64\n",
    "    )\n",
    "    # Optimize parameters in backbone and in objective using single optimizer.\n",
    "    params = itertools.chain(model.parameters(), objective.parameters())\n",
    "    optimizer = SGD(params=params, lr=lr, momentum=0.9)\n",
    "    min_lr = optimizer.defaults.get(\"lr\") * 1e-3\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=min_lr)\n",
    "    \n",
    "    # set_seed(0)\n",
    "    torch.manual_seed(42)\n",
    "    trainer = BasicTrainer(\n",
    "        dataset=x_train, \n",
    "        model=model,\n",
    "        objective=objective,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=None,\n",
    "        batch_size=batch_size,\n",
    "        accumulation_steps=2,\n",
    "        num_workers=2,\n",
    "        epochs=100,\n",
    "        device='cuda',\n",
    "    )\n",
    "\n",
    "    \n",
    "    if os.path.exists(\"/home/wellvw12/fed_wild/checkpoint.pth\"):\n",
    "        trainer.load(\"checkpoint.pth\")\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save(\"./\",\"checkpoint.pth\",False)\n",
    "    \n",
    "    # Metadata needed for aggregation server side\n",
    "    \n",
    "    metadata = {\n",
    "        # num_examples are mandatory\n",
    "        \"num_examples\": len(x_train),\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": lr,\n",
    "    }\n",
    "\n",
    "    # Save JSON metadata file (mandatory)\n",
    "    save_metadata(metadata, out_model_path)\n",
    "\n",
    "    # Save model update (mandatory)\n",
    "    save_parameters(model, out_model_path)\n",
    "\n",
    "def validate(in_model_path, data_path=None):\n",
    " \n",
    "    x_query = full_data(data_path + \"query.csv\")\n",
    "    x_gallery  = full_data(data_path + \"gallery.csv\", is_query=False)\n",
    "\n",
    "    # Load model\n",
    "    model = load_parameters(in_model_path)\n",
    "    model.eval()\n",
    "\n",
    "    extractor = DeepFeatures(model)\n",
    "    \n",
    "    query = extractor(x_query)\n",
    "    database = extractor(x_gallery)\n",
    "\n",
    "    similarity_function = CosineSimilarity()\n",
    "    similarity = similarity_function(query, database)\n",
    "\n",
    "    classifier = KnnClassifier(k=1, database_labels=x_gallery.labels_string)\n",
    "    predictions = classifier(similarity['cosine'])\n",
    "    accuracy = np.mean(x_query.labels_string == predictions)\n",
    "\n",
    "    print(f'accuracy: {accuracy}')\n",
    "\n",
    "    # JSON schema\n",
    "    report = {\n",
    "        \"training_accuracy\": accuracy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wellvw12/data_iid/clients/1/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████| 17/17 [00:05<00:00,  3.07it/s]\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.23it/s]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.20it/s]\n",
      "Epoch 3: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.21it/s]\n",
      "Epoch 4: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.19it/s]\n",
      "Epoch 5: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.19it/s]\n",
      "Epoch 6: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.16it/s]\n",
      "Epoch 7: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.17it/s]\n",
      "Epoch 8: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.16it/s]\n",
      "Epoch 9: 100%|██████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 10: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.16it/s]\n",
      "Epoch 11: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 12: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.11it/s]\n",
      "Epoch 13: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 14: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 15: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.09it/s]\n",
      "Epoch 16: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.08it/s]\n",
      "Epoch 17: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.10it/s]\n",
      "Epoch 18: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.10it/s]\n",
      "Epoch 19: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.11it/s]\n",
      "Epoch 20: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 21: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 22: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 23: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 24: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.16it/s]\n",
      "Epoch 25: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 26: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 27: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 28: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 29: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 30: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 31: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 32: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 33: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.16it/s]\n",
      "Epoch 34: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 35: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 36: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 37: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 38: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 39: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 40: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 41: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 42: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 43: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.10it/s]\n",
      "Epoch 44: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.11it/s]\n",
      "Epoch 45: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 46: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 47: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 48: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 49: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 50: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.16it/s]\n",
      "Epoch 51: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 52: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 53: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 54: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 55: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 56: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 57: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 58: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.16it/s]\n",
      "Epoch 59: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 60: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 61: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 62: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 63: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 64: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 65: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 66: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.11it/s]\n",
      "Epoch 67: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 68: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 69: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 70: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.10it/s]\n",
      "Epoch 71: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.10it/s]\n",
      "Epoch 72: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 73: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 74: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 75: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.15it/s]\n",
      "Epoch 76: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.14it/s]\n",
      "Epoch 77: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 78: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 79: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 80: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 81: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 82: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 83: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.12it/s]\n",
      "Epoch 84: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.11it/s]\n",
      "Epoch 85: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.08it/s]\n",
      "Epoch 86: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.86it/s]\n",
      "Epoch 87: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.90it/s]\n",
      "Epoch 88: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.90it/s]\n",
      "Epoch 89: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.91it/s]\n",
      "Epoch 90: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.88it/s]\n",
      "Epoch 91: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.85it/s]\n",
      "Epoch 92: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.97it/s]\n",
      "Epoch 93: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.93it/s]\n",
      "Epoch 94: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.95it/s]\n",
      "Epoch 95: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.95it/s]\n",
      "Epoch 96: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  3.91it/s]\n",
      "Epoch 97: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n",
      "Epoch 98: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.10it/s]\n",
      "Epoch 99: 100%|█████████████████████████████████████████████████████| 17/17 [00:04<00:00,  4.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "train('/home/wellvw12/fed_wild/project/seed.npz','/home/wellvw12/fed_wild/project/seedZ.npz',data_path='/home/wellvw12/data_iid/clients/1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wellvw12/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wellvw12/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/var/tmp/ipykernel_15816/2930594771.py:257: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n",
      "100%|█████████████████████████████████████████████████████████████████| 6/6 [00:46<00:00,  7.79s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 34/34 [04:24<00:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.041666666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/wellvw12/venv/lib/python3.10/site-packages/wildlife_tools/inference/classifier.py:61: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  results = pd.DataFrame(results).T.fillna(method=\"ffill\").T\n"
     ]
    }
   ],
   "source": [
    "validate('/home/wellvw12/resnet50_ft_net.pth','/home/wellvw12/full_leopard_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wellvw12/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m     li\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m---> 29\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mli\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([T\u001b[38;5;241m.\u001b[39mResize([\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m]), \n\u001b[1;32m     31\u001b[0m                        T\u001b[38;5;241m.\u001b[39mToTensor(), \n\u001b[1;32m     32\u001b[0m                        T\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m), std\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m)),\n\u001b[1;32m     33\u001b[0m                        T\u001b[38;5;241m.\u001b[39mRandomResizedCrop(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)),\n\u001b[1;32m     34\u001b[0m                     ])\n\u001b[1;32m     35\u001b[0m d \u001b[38;5;241m=\u001b[39m WildlifeDataset(frame,metadata\u001b[38;5;241m.\u001b[39mroot, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from wildlife_tools.data.dataset import WildlifeDataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Set your root directory\n",
    "root_dir = '/home/wellvw12/data_iid'\n",
    "metadata = '/home/wellvw12/Cows'\n",
    "\n",
    "all_rows = []\n",
    "columns = None\n",
    "file_paths = []\n",
    "\n",
    "# Walk through all directories and files\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # print(dirpath,dirnames,filenames)\n",
    "    for filename in filenames:\n",
    "        # print(filename)\n",
    "        if filename.lower() == 'test.csv':\n",
    "            # print(filename)\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "li = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "transform = T.Compose([T.Resize([224, 224]), \n",
    "                       T.ToTensor(), \n",
    "                       T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                       T.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "                    ])\n",
    "d = WildlifeDataset(frame,metadata.root, transform=transform)\n",
    "# Create DataFrame manually\n",
    "# df = pd.DataFrame(all_rows, columns=columns)\n",
    "frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wellvw12/data_iid/clients/1/test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "131 367\n"
     ]
    }
   ],
   "source": [
    "query_df , gallery_df = load_data('/home/wellvw12/data_iid/clients/1/' + \"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
